{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srikanth-datascientist/sriGit/blob/main/Extract_Text_From_PDF_and_search_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de5d6e0c",
      "metadata": {
        "toc": true,
        "id": "de5d6e0c"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#https://stackabuse.com/working-with-pdfs-in-python-reading-and-splitting-pages/\" data-toc-modified-id=\"https://stackabuse.com/working-with-pdfs-in-python-reading-and-splitting-pages/-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span><a href=\"https://stackabuse.com/working-with-pdfs-in-python-reading-and-splitting-pages/\" target=\"_blank\">https://stackabuse.com/working-with-pdfs-in-python-reading-and-splitting-pages/</a></a></span></li><li><span><a href=\"#Hybrid-Approach-for-Fuzzy-Name-Matching\" data-toc-modified-id=\"Hybrid-Approach-for-Fuzzy-Name-Matching-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Hybrid Approach for Fuzzy Name Matching</a></span><ul class=\"toc-item\"><li><span><a href=\"#Common-Issues:\" data-toc-modified-id=\"Common-Issues:-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Common Issues:</a></span></li></ul></li><li><span><a href=\"#Lets-start-Fuzzy-matching-in-python---Method1\" data-toc-modified-id=\"Lets-start-Fuzzy-matching-in-python---Method1-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Lets start Fuzzy matching in python - Method1</a></span></li><li><span><a href=\"#N-Grams\" data-toc-modified-id=\"N-Grams-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>N-Grams</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>TF-IDF</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF:\" data-toc-modified-id=\"TF:-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>TF:</a></span></li><li><span><a href=\"#IDF:\" data-toc-modified-id=\"IDF:-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>IDF:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Note:-it-is-safe-to-use-df+1-in-the-formula-to-handle-cases-when-df=0-and-avoid-division-by-zero-error.\" data-toc-modified-id=\"Note:-it-is-safe-to-use-df+1-in-the-formula-to-handle-cases-when-df=0-and-avoid-division-by-zero-error.-5.2.0.1\"><span class=\"toc-item-num\">5.2.0.1&nbsp;&nbsp;</span>Note: it is safe to use df+1 in the formula to handle cases when df=0 and avoid division by zero error.</a></span></li></ul></li></ul></li><li><span><a href=\"#TF-IDF:\" data-toc-modified-id=\"TF-IDF:-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>TF-IDF:</a></span></li></ul></li><li><span><a href=\"#Cosine-Similarity\" data-toc-modified-id=\"Cosine-Similarity-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Cosine Similarity</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Lets-start-Fuzzy-matching-in-python---Method2\" data-toc-modified-id=\"Lets-start-Fuzzy-matching-in-python---Method2-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Lets start Fuzzy matching in python - Method2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Common-Issues:\" data-toc-modified-id=\"Common-Issues:-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Common Issues:</a></span></li></ul></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>TF-IDF</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF:\" data-toc-modified-id=\"TF:-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>TF:</a></span></li><li><span><a href=\"#IDF:\" data-toc-modified-id=\"IDF:-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>IDF:</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Note:-it-is-safe-to-use-df+1-in-the-formula-to-handle-cases-when-df=0-and-avoid-division-by-zero-error.\" data-toc-modified-id=\"Note:-it-is-safe-to-use-df+1-in-the-formula-to-handle-cases-when-df=0-and-avoid-division-by-zero-error.-9.2.0.1\"><span class=\"toc-item-num\">9.2.0.1&nbsp;&nbsp;</span>Note: it is safe to use df+1 in the formula to handle cases when df=0 and avoid division by zero error.</a></span></li></ul></li></ul></li><li><span><a href=\"#TF-IDF:\" data-toc-modified-id=\"TF-IDF:-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>TF-IDF:</a></span></li></ul></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "atomic-zoning",
      "metadata": {
        "id": "atomic-zoning"
      },
      "source": [
        "### https://stackabuse.com/working-with-pdfs-in-python-reading-and-splitting-pages/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7RoiZiksRhJ",
        "outputId": "3c447cdd-6519-4a1d-81bd-02e32e40f56c"
      },
      "id": "A7RoiZiksRhJ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-2.2.1-py3-none-any.whl (189 kB)\n",
            "\u001b[K     |████████████████████████████████| 189 kB 9.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from PyPDF2) (4.1.1)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BTZQ8HLVtWHZ",
        "outputId": "6ef8091e-0dc3-49d0-b73d-c4379fc39009"
      },
      "id": "BTZQ8HLVtWHZ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textract\n",
            "  Downloading textract-1.6.5-py3-none-any.whl (23 kB)\n",
            "Collecting argcomplete~=1.10.0\n",
            "  Downloading argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
            "Collecting six~=1.12.0\n",
            "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting xlrd~=1.2.0\n",
            "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 27.9 MB/s \n",
            "\u001b[?25hCollecting docx2txt~=0.8\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Collecting python-pptx~=0.6.18\n",
            "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 39.7 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4~=4.8.0\n",
            "  Downloading beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 38.2 MB/s \n",
            "\u001b[?25hCollecting SpeechRecognition~=3.8.1\n",
            "  Downloading SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8 MB 97 kB/s \n",
            "\u001b[?25hCollecting extract-msg<=0.29.*\n",
            "  Downloading extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from textract) (3.0.4)\n",
            "Collecting pdfminer.six==20191110\n",
            "  Downloading pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 34.8 MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "  Downloading pycryptodome-3.14.1-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4~=4.8.0->textract) (2.3.2.post1)\n",
            "Collecting imapclient==2.1.0\n",
            "  Downloading IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting compressed-rtf>=1.0.6\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "Collecting tzlocal>=2.1\n",
            "  Downloading tzlocal-4.2-py3-none-any.whl (19 kB)\n",
            "Collecting olefile>=0.46\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 62.4 MB/s \n",
            "\u001b[?25hCollecting ebcdic>=1.1.1\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from python-pptx~=0.6.18->textract) (4.2.6)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from python-pptx~=0.6.18->textract) (7.1.2)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "  Downloading XlsxWriter-3.0.3-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 67.0 MB/s \n",
            "\u001b[?25hCollecting pytz-deprecation-shim\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting backports.zoneinfo\n",
            "  Downloading backports.zoneinfo-0.2.1-cp37-cp37m-manylinux1_x86_64.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 9.0 MB/s \n",
            "\u001b[?25hCollecting tzdata\n",
            "  Downloading tzdata-2022.1-py2.py3-none-any.whl (339 kB)\n",
            "\u001b[K     |████████████████████████████████| 339 kB 58.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: docx2txt, compressed-rtf, olefile, python-pptx\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=4720593f91304c4f884f959736232348ad96a0a143c4b748077959a4648ddcfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6204 sha256=02bd4bf5eae3ef9167e3ae4f237eb5c7cda2430041ad9bc5553b2682d7d72363\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/33/88/88ceee84d1b74b391c086bc594d3fcf80800decfbd6e1ff565\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35432 sha256=2d2436eaea65486a27d1795a319d8679eeba124292c9a55acf646aefd369aeff\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/53/e6/37d90ccb3ad1a3ca98d2b17107e9fda401a7c541ea1eb6a65a\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470951 sha256=c94bc60a2d3c19aed68041643973fb4621d29762103ea946d8a087f589cefc28\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/ab/f4/52560d0d4bd4055e9261c6df6e51c7b56c2b23cca3dee811a3\n",
            "Successfully built docx2txt compressed-rtf olefile python-pptx\n",
            "Installing collected packages: tzdata, backports.zoneinfo, six, pytz-deprecation-shim, XlsxWriter, tzlocal, pycryptodome, olefile, imapclient, ebcdic, compressed-rtf, xlrd, SpeechRecognition, python-pptx, pdfminer.six, extract-msg, docx2txt, beautifulsoup4, argcomplete, textract\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 1.5.1\n",
            "    Uninstalling tzlocal-1.5.1:\n",
            "      Successfully uninstalled tzlocal-1.5.1\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-python-client 1.12.11 requires six<2dev,>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "google-api-core 1.31.6 requires six>=1.13.0, but you have six 1.12.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed SpeechRecognition-3.8.1 XlsxWriter-3.0.3 argcomplete-1.10.3 backports.zoneinfo-0.2.1 beautifulsoup4-4.8.2 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.14.1 python-pptx-0.6.21 pytz-deprecation-shim-0.1.0.post0 six-1.12.0 textract-1.6.5 tzdata-2022.1 tzlocal-4.2 xlrd-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xM40mVJtZLt",
        "outputId": "acc1ad79-68bd-48fa-9160-bf3cbc31c86d"
      },
      "id": "9xM40mVJtZLt",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "level-utilization",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "level-utilization",
        "outputId": "94c609b5-bdbd-49f7-8958-0646157c1eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import PyPDF2 \n",
        "import textract\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fuzzy-money",
      "metadata": {
        "id": "fuzzy-money"
      },
      "outputs": [],
      "source": [
        "#Write a for-loop to open many files (leave a comment if you'd like to learn how).\n",
        "#filename = 'test.pdf' \n",
        "def extractText(filename):\n",
        "    #open allows you to read the file.\n",
        "    pdfFileObj = open(filename,'rb')\n",
        "    #The pdfReader variable is a readable object that will be parsed.\n",
        "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
        "    #Discerning the number of pages will allow us to parse through all the pages.\n",
        "    num_pages = pdfReader.numPages\n",
        "    count = 0\n",
        "    text = \"\"\n",
        "    #The while loop will read each page.\n",
        "    while count < num_pages:\n",
        "        pageObj = pdfReader.getPage(count)\n",
        "        count +=1\n",
        "        pgTxt = pageObj.extractText()\n",
        "        if pgTxt.strip() !=\"\":\n",
        "            text += pgTxt\n",
        "    #This if statement exists to check if the above library returned words. It's done because PyPDF2 cannot read scanned files.\n",
        "    if text.strip() != \"\":\n",
        "       text = 'PDF '+text\n",
        "    #If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text.\n",
        "    else:\n",
        "       text = textract.process(filename, method='tesseract', language='eng')\n",
        "    #Now we have a text variable that contains all the text derived from our PDF file. Type print(text) to see what it contains. It likely contains a lot of spaces, possibly junk such as '\\n,' etc.\n",
        "    #Now, we will clean our text variable and return it as a list of keywords.\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "champion-start",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "champion-start",
        "outputId": "51773f4f-fad6-4210-83e8-de7fb728eaa6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b4fbcb76e67e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextractText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.pdf'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-b2dc19d338a3>\u001b[0m in \u001b[0;36mextractText\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextractText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#open allows you to read the file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpdfFileObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#The pdfReader variable is a readable object that will be parsed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpdfReader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPdfFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdfFileObj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.pdf'"
          ]
        }
      ],
      "source": [
        "print(extractText('test.pdf' ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "married-processing",
      "metadata": {
        "id": "married-processing"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import timedelta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abandoned-insured",
      "metadata": {
        "id": "abandoned-insured",
        "outputId": "5b9135af-fed2-4ce3-bc10-ce433283330d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:00:10.431647\n",
            "CPU times: user 8.58 s, sys: 544 ms, total: 9.13 s\n",
            "Wall time: 10.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "start_time = time.monotonic()\n",
        "text = extractText('MasterPython.pdf' )\n",
        "end_time = time.monotonic()\n",
        "print(timedelta(seconds=end_time - start_time))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "damaged-assets",
      "metadata": {
        "id": "damaged-assets",
        "outputId": "4024abc4-b654-441a-83a8-650b3d117dac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'PDF PYTHON MACHINE LEARNING\\nLearn Python in a Week and Master it\\n7 Days Crash Course\\nAn hands-on int'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#text = text.decode('UTF-8')\n",
        "text[0:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pregnant-minimum",
      "metadata": {
        "id": "pregnant-minimum"
      },
      "outputs": [],
      "source": [
        "#The word_tokenize() function will break our text phrases into individual words.\n",
        "tokens = word_tokenize(text)\n",
        "#We'll create a new list that contains punctuation we wish to clean.\n",
        "punctuations = ['(',')',';',':','[',']',',']\n",
        "#We initialize the stopwords variable, which is a list of words like \"The,\" \"I,\" \"and,\" etc. that don't hold much value as keywords.\n",
        "stop_words = stopwords.words('english')\n",
        "#We create a list comprehension that only returns a list of words that are NOT IN stop_words and NOT IN punctuations.\n",
        "keywords = [word.lower() for word in tokens if not word in stop_words and not word in punctuations]\n",
        "\n",
        "#used.append(x) for x in mylist if x not in used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rural-retreat",
      "metadata": {
        "id": "rural-retreat",
        "outputId": "1fc8ed8e-a516-49d3-8622-6bc1d0324278"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['pdf', 'python', 'machine', 'learning', 'learn', 'python', 'week', 'master', '7', 'days']\n"
          ]
        }
      ],
      "source": [
        "print(keywords[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "great-homeless",
      "metadata": {
        "id": "great-homeless",
        "outputId": "53dfef2e-8067-4bc4-fd13-4a5e8e370915"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "23600"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "divine-contents",
      "metadata": {
        "id": "divine-contents",
        "outputId": "ba761482-0f2a-4a0c-c8e1-dfb6fa1a3146"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4156"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Unique keywords\n",
        "len(list(set(keywords)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "random-consistency",
      "metadata": {
        "scrolled": true,
        "id": "random-consistency",
        "outputId": "9dcead91-e70f-4112-a02b-ef2c5338007f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'perfect', 'flipping', 'method', 'wherein', 'graph', 'major', 'empowering', 'sns.despine', 'fine', 'fix', 'strength', 'sector', 'part', 'estimate', 'format', 'comprehensible', 'complicated', 'computer', 'modeled', 'chart', 'cutting', \"loss='sparse_categorical_cr\", 'preferred', 'large', 'mris', 'numeric', 'similar', 'increase', 'clustering', 'sales_data.tail', 'bigger', 'opposite', 'open', 'growth', 'incurred', 'online', 'member', 'unsupervised', 'mistake', 'exists', 'cancer', 'tailored', 'project', 'errors', 'plugged', 'locally', 'dairy', 'advice', 'proxies', 'drug', '777777', '1991', '}', 'reinforcement', 'pheromone', 'yes', 'calculation', 'checkpoints', 'determined', 'modifying', 'practical', 'increasing', 'win', 'separation', 'passing', 'rainfall-runoff', 'analyzes', 'rated', 'conventional', '1/5', 'then', '1959', 'separate', 'label', 'created', 'bathrooms', 'self-organization', 'kit', 'christmas', 'draws', 'high-', '0.5', 'toddler', 'watermelon', 'assumption', 'paradigms', '3-d', \"let's\", 'optimal', 'underground', 'maneuvering', 'stopping', 'ounces', 'aggregation', 'four', 'interacting', 'comments', 'frequent', 'identification', 'involves', 'compiled', 'seen', 'lot', 'centers', 're-use', 'dictates', 'p-value', 'adding', '4s', 'exchanging', 'notes', 'all', 'memberships', 'considerably', 'removed', 'mind', 'spills', 'titles', 'additional', 'executives', 'img.shape', 'contained', 'effective.', '25', 'margins', 'performance', 'some', 'modeling', 'ridge', 'never', 'implementations', 'said', 'skillset', 'widelyutilized', 'categorize', 'specific', 'addition', 'reduce', '0.0', 'detected', 'roc_curv', '7-day-course', 'improving', 'web', 'printf', 'computing', 'speak', 'measuring', 'avoided', 'naive', 'energy', 'ambiguous', 'central', 'met', 'imperativeness', 'accrued', 'def', 'destination', 'scatter', 'privacy', 'ecosystem', 's-shaped', 'want', 'runs', 'calculate', 'observations', 'cube', '4.6347316e-02', 'expansion', 'procure', 'off-spring', 'operates', 'revisits', 'states', 'x', 'raining', 'selecting', 'meeting', 'match', 'belief', 'broken', 'reportedly', 'current', 'concerns', 'decade', 'indicate', 'intervention', 'accordingly', 'despite', 'complete', 'assessment', \"'dress\", 'conditions', 'attempted', 'assure', 'adjust', 'snippet', 'programs', 'predictions_array', 'flag', 'reproduced', 'values', 'consisting', 'theorem', 'assigns', 'collaborative', 'prices', 'primary', \"'pullover\", 'probable', '2020', 'integers', 'initialize', 'controlled', 'boosting', 'revelation', 'obtaining', \"n't\", '...', '{', 'luxury', 'pants', 'nodes', 'comparison', 'campaign', 'activity', 'towards', 'graphics', 'related', 'higher', 'designed', 'symbolic', 'toprevent', 'assumes', 'six', 'points:1', 'carrier', 'conditional', 'smell', 'mentioned', 'intr', 'martin', 'blue', 'classes', 'gold', 'supported', 'borrowed', 'predefined', 'supersets', 'input', 'probabilistic', 'professional', 'iso', 'bake', 'corresponding', 'water-quality', '3.1', 'mx', \"'sneaker\", 'retail', 'german', 'country', '0.3138', 'world', 'direct', 'reading', 'apparel', 'issue', 'savings', 'train_images', '0', '.however', 'security', 'on.quantitative', 'moment', 'refers', 'bar', 'connected', 'fields', 'compared', 'otherwise', 'automatathe', 'possibly', 'challenging', 'predicted', 'production', \"'blue\", 'side-ef', 'interest', '0.0000453978687', 'last', 'process.discretization', 'tensorflow.python.keras.callbacks.history', 'economic', 'test_images', 'prior', 'answer', 'effort', 'denote', 'pre-process', 'classifying', 'contributions', 'handling', 'cognitive', 'company', 'valley', 'chromosome', 'script', 'blackberry', 'predicting', 'engineering', 'prompt', 'flip', 'inputs', 'groundwater', 'vote', 'include', 'feel', '60,000', 'bsd', 'epoch', '1.5', 'community', '2', 'whose', 'groups', 'almonds', 'presentation', 'train', 'matrix', 'splitting', 'phones', 'student', 'note', 'discount', 'reader', 'careful', 'around', 'separates', 'twitter', 'plot_image', 'toolkit', 'compression', 'obviously', 'belonging', '8', 'columns', 'differences', 'warranted', '12', 'iterations', 'participation', 'drives', 'ratios', 'drivers', 'inclined', 'keywords', 'please', 'containthree', 'i.e', 'ytrain', 'candidate', 'ease', 'abundance', 'included', 'kernel', 'pca', 'tests', 'okyo', 'mastered', 'study', 'fishing', 'item', '11.7px', 'famous', 'neuro-fuzzy', 'investigating', 'sns.countplot', 'lazy', 'loads', 'cellular', 'beta', 'heavy', 'intensive', 'randomstate', 'each', 'distinguish', 'networks', 'concepts', '5-star', 'recall_score', 'take', 'however', 'accept', 'after', 'exploring', 'enterprise', 'countplot', 'still', 'attempting', 'sat', 'retrieving', 'updated', 'detail.regression', 'combinatorial', 'evolve', '0.12.3', 'york', 'programming', 'true', '%', 'info', 'r', 'soil', 'self-learning', 'noise', 'discrete', 'assessing', 'fixing', 'concealed', 'gather', 'onwhether', 'inside', 'remove', 'classification7', 'pattern', 'intended', 'unrelated', 'manufacturer', 'launch', 'statements', 'creates', 'survey', 'linearly', 'optimizers', 'view', '4.17183337e-08', 'spoken', 'entertainment', 'labrador', 'automates', 'start', 'layer', 'processing', 'error', 'shot', 'standardscale', 'confidence', \"'accuracy\", 'length', 'computation', 'text', 'fashion_mnist', 'footage', 'building', 'controller', 'hackers', 'names', 'train_size', 'ficiency', 'concerning', 'criteria', 'tf.keras.layers.dense', 'shepherd', 'dependably', 'constructed', 'becomes', 'ultimate', '1-', 'oil', 'pure', 'one-shot', 'abstract', \"'sandal\", 'svm', 'research', 'accurate', 'cost-effective', 'distinction', '``', 'sn', 'applying', 'drastically', 'carnegie', 'reliable', 'layers', \"'ankle\", 'distributions', 'elbow', 'float', 'food', '255.0', 'exciting', 'depositing', 'emails', 'no', 'depictions', 'taught', 'rates', 'journey', 'derivatives', 'wealth', 'before', 'overview', 'loaded', 'form', 'apply', 'corner', 'row2', 'meet', 'tv', 'advancement', 'procured', 'assigned', 'unity', 'against.', 'facebook', 'test_acc', '/', 'style', 'plots', 'rbs', 'engines', 'terms', 'costly', 'produced', 'accuracy', 'height', 'everything', 'misclassified', 'demand', 'concentration', 'application', 'gradient', 'written', '20-40', '1,3', 'plain', 'possibilities', 'discovery', 'split', 'must', '5/', 'fitted', 'extracted', 'crucial', 'long', 'together', 'endorsements', 'monitoring', 'yet', 'shorter', 'walk', 'saving', 'keras.sequential', 'sufficient', 'else', 'looking', 'deleted', 'getting', 'pages', 'netscape', 'stronger', 'ogramming', 'card', 'credit', 'k.', 'sourced', 'spread', 'assigning', 'berkley', 'accomplished', 'support', 'cameras', 'identifiers', 'watson', 'occasionally', 'record', 'released', 'sklearn.preprocessing', 'math', 'floods', 'america', 'hardware', 'introduce', 'codes', 'stocks', 'election', 'implicit', 'average', 'begin', 'match-', 'neighbor', 'matplotlib.pyplot', 'tools', 'deal', 'supplier', 'techniques', 'oak', 'operated', 'try', 'linear', 'promotions', 'precision_score', 'abilities', 'statement', 'leading', 'm', 'teach', 'prefer', 'directions', 'observed', 'and/or', 'birds', 'suggestions', 'verify', 'sales_data.dtypes', 'studying', 'respective', 'recording', 'etc.', 'sigmoid', 'modify', 'machine_learning_map', 'reparation', 'chaining', 'times', 'management', 'target_test', 'transaction', '0.3', 'compute', 'developing', 'personalized', 'suites', 'increases', 'u.s', 'dig', '16', 'within', 'tons', 'hoping', 'solving', 'individuals', 'trends', 'time-', 'recorded', 'accessibility', 'brain', 'development', 'lifestyle', 'x-transpose', 'colony', 'cleaning', 'agents', 'greatest', 'ingredient', 'searching', 'certified', 'data.head', 'evaluation', 'functions', 'insight', 'col0', 'responsibility', 'effectively', 'joint', 'japanese-french', 'needs', 'speech', 'inadequacies', 'junk', '0.89', 'identify', 'accessed', 'non-', 'legal', '.format', 'protected', 'robust', 'documented', 'unison', 'supervisory', 'board', 'blame', 'comprises', 'apparent', 'predict', '1.11.0', '~', 'orchestrate', 'animal', 'independently', 'observingthe', 'predominantly', 'firmware', 'resemble', 'experienced', 'configuring', 'principles', 'users', 'functionality', 'interactive', 'conceptual', 'federated', 'certainty', 'attribute', 'kneighborsclassifier', 'face', 'fit', 'powerhouse', 'count', 'simple', 'deliver', 'self', 'suppose', 'cost', 'used', 'capable', 'model.evaluate', 'duplicated', 'author', 'absorbed', 'below', 'numerical', 'lar', 'lacking', 'memory-', 'symbol', 'pioneering', 'eradicated', 'whitegrid', 'fit_transform', 'cpu', 'rule', 'df.head', 'naur', 'inefficient', 'membership', 'decision-making', 'hard', 'use', 'node', 'since', 'emanates', 'automata', 'in', 'real-world', 'traits', '28', 'subsequently', '784', 'coined', 'pouring', 'retrieval', 'rights', 'extensive', 'k-nearest', 'logistics', 'transactional', 'makes', 'console', 'multitude', 'custom', 'tf.keras.layers.flatten', 'robotic', 'deliverables', 'keeps', 'thank', 'stanford', '500', 'get_dummies', 'cmap=plt.cm.binary', 'tweets', 'collecting', 'outliers', 'integrated', 'touted', 'financial', 'search', 'quantify', 'equations', 'constructing', 'rc=', 'zero', 'np.expand_dims', '2010', 'ɛ', 'flexibility', 'comparable', 'supplied', 'professionals', 'forget', 'oriented', 'protocols', 'expression', 'likely', 'many', 'recovered', 'qualities', 'vegetation', 'touched', 'depth', '65:35', 'imported', 'daily', 'loops', 'potential', 'domain', 'providedthat', 'fresh', 'plot_value_array', 'language', '1000', 'lowers', 'subsets', 't', 'refer', 'consistently', 'statistician', 'ambitious', 'qualitative', 'examples', 'land', 'congratulations', 'governing', 'costs', \"'trouser\", 'iphone', 'reviews', 'other', 'books', 'month', 'yellow', '’', 'timeliness', 'complexity', 'dimensional', 'svc', 'second', 'frequently', 'refresh', 'strongest', 'compilation', 'instance', 'fromand', 'byte', 'centimeters', 'atmospheric', 'fast', 'recognition', 'general', 'using', 'ossentropy', 'comes', 'small', 'coefficient', 'supervise', 'dynamic', 'clean', 'weak', 'expert', 'woman', 'external', 'notion', 'conflict', 'suits', 'meaning', 'poor', 'cause', 'replaced', 'hybrid', 'laws', 'agrees', 'admission', 'substitute', 'usedterminology', 'how', 'well', 'abundantly', 'obvious', 'considering', 'provided', 'ready', 'it.encoding', 'plt.colorbar', 'create', 'science', 'indirect', 'fuzzification', 'reproduction', 'scikit-image', 'comma', '.', 'wide', 'action', 'curve', 'needing', 'random_state=0', 'display', 'linking', 'simplicity', 'fake', 'sessions', 'unique', 'asynchronously', 'agentsare', 'knowledge', 'contents', 'held', 'swarm', 'named', 'hazards', 'instruct', 'reference', 'stuck', 'marketing', 'solely', 'relations', 'ideas', 'earthquake', 'feasible', 'randomly', 'thedata', 't-stochastic', 'variance', 'analyzing', 'beagle', 'tao', \"'random\", 'configured', 'addressing', 'exit', 'enrollment', 'np.max', 'white', 'devices', 'extension', 'difficult', 'high-quality', 'pip', 'drop-in', 'series', 'correlated', 'faster', '4', 'architectures', 'specifications', 'adapt', 'directories', 'fed', 'housing', 'tendency', 'reduction', 'technique', 'combined', 'tabular', 'false', 'alignment', 'job', 'predicted_label', 'entire', 'institutions', 'separated', 'divides', 'classifies', 'thorough', 'booleans', 'appear', 'original', 'pixel', 'scikit-learn', 'inferred', 'seven', 'solve', 'formula', 'extend', 'n2', 'combing', 'thumb', 'train_test_split', \"'figure.figsize\", 'continuously', 'efficiencies', 'implementation', 'median', 'char', 'minor', 'printing', 'xp', 'today', 'basics', 'excellent', 'calendar', 'goals', 'texts', 'definitions', 'symbols', 'intricately', 'occur', 'constituents', '0.3763', 'introduces', '.net', 'closely', 'precise', 'larger', 'due', 'distribution', 'holds', 'parent', 'allow', \"'s\", 'advance', 'manually', '—', 'resembles', 'good', 'objectives', 'capturing', 'truck', 'minimum', 'floating', '800', 'discussed', 'possess', 'produce', 'matcher', 'environmental', 'preparation', 'sometimes', 'bulk', 'alter', 'determine', 'glance', 'statistics-based', 'microsystems', 'recalling', '72us/sample', 'consists', 'enabling', 'enrolment', 'falling', 'journalists', 'operations', 'maintaining', 'easy-to-use', 'associated', 'packages', 'raw', 'flattened', 'transmitted', 'choosing', 'hearing', 'connection', 'integer', 'discarded', 'livescript', 'adjacent', 'rewrite', 'utilizing', 'combine', 'frequentist', 'displayed', 'someone', 'scientist', 'abstraction', 'changes', 'joblib', 'attack', 're-calculates', 'reported', 'almost', 'encodes', 'plot_', 'g', 'washington', 'setup', 'dictionaries', 'x+b', 'mathematics', 'nearing', 'lowered', 'already', 'sales_data.columns', 'consuming', 'logs', 'visualize', 'structured', 'working', 'particular', 'relatively', 'dropping', 'disadvantages', 'guide', 'consequently', 'damages', 'elected', 'probability', 'provides', 'except', 'parameter', 'confused', 'wary', 'effectiveness', 'houses', 'handle', 'course', 'stable', 'binning', 'constantly', 'rules', 'wrongly', 'word2vec', 'supports', 'papers', 'specified', 'minimize', 'sklearn.neighbors', 'subject', 'relationships', 'california', 'water', 'adjusting', 'flocks', 'appropriate', 'c++', 'summer', 'applicability', 'irreducible', 'scatterplot', 'pipelineto', 'communication', 'releasing', 'helpful', 'intellectual', 'diagnosis', 'optimizers.basic', 'preprocessing.labelencoder', 'enables', 'visualizations', 'car', 'adjusts', 'content/uploads/2015/04/w', 'two-dimensional', 'nearer', 'popularly', 'concise', 'classification', 'keeping', 'readily', 'diameter', 'prevents', 'helping', 'introduced', 'platform', 'rich', \"'coat\", 'forests', '2.3', 'value', 'moving', 'equipment', 'multicollinearity', 'branch', 'computed', 'compatible', 'ago', 'upsell', 'preview', 'improve', '.c', 'parameters', 'overfitting', '89', 'replacing', 'bees', 'targets', 'density', 'leveraging', 'something', \"'route\", 'run', 'message', \"'ggplot\", 'publisher', 'extremely', 'interpreter', 'elaborate', 'gives', 'several', '0.6', 'authorities', 'ongoing', 'anticipated', 'so', 'sklearn.externals', 'an', 'pyplot', 'capability', 'core', 'grow', 'initiatives', 'pollution', 'allocation', 'semi-structured', 'classification_report', 'percent', 'β1x1', 'sensitivity', 'alters', 'projecting', 'cookies', 'indirectly', '-plane', 'lotus', 'quiz', 'academyall', 'successfully', 'treatment', 'divided', 'know', 'largely', 'misinterpreted', 'previous', 'powers', 'massive', 'observing', 'scikit-', 'calculations', 'supervised', 'forms', 'appliances', 'summary', 'outside', 'extent', 'kilometers', 'driving', 'depiction', 'rapid', 'backward', 'valuation', 'sharp', 'shown', 'computers', 'evaluation.designed', 'model.fit', '51us/sample', 'primarily', 'composed', 'distinct', 'stage', 'satisfy', 'random_state', 'principal', '1.5.1', 'micr', '3.5', 'chess', '101', 'construction', 'implying', 'administered', 'algorithms', 'editing', 'challenge', 'easiest', 'jsp', 'frame', 'mean', 'wholesale', 'relevance', 'aircraft', 'carriers', 'sends', 'numerosity', 'single', 'buffer', 'operation', 'quality', 'reinforced', 'thousands', 'three', 'deep', 'fashion', 'b0=', 'fects', 'verified', 'book', 'round', 'smoother', 'customers', 'add', '6', 'needed', 'continues', 'subset', 'neuron', 'stigmergy', 'brings', 'dive', 'self-organizing', 'secure', 'spreadsheets', '128', 'price', 'extra', 'impossible', 'planes', 'consumption.', 'checked', 'tends', 'figure', 'concurrent', 'keyword', 'normally', 'maps', 'samples', 'entering', 'green', 'bill', 'might', 'worth', 'estimation', 'matching', 'strengths', 'tornadoes', 'interested', 'assorted', 'banks', 'eclipse', 'suited', 'definition', '0.8777epoch', 'otation=45', 'real-valued', 'spelling', 'published', 'adapts', '2d/3d', 'material', 'entry', 'personal', 'phrases', 'cournapeau', 'boasting', 'relative', 'purchasing', 'category', 'shows', 'ecological', 'kind', 'ytest', 'x2', 'particle', 'functional', 'quickly', 'entitled', 'p', 'functionalities', 'ignored', 'critical', 'year', 'verification', 'waste', 'user', 'fish', 'outlined', 'mechanism', 'bought', 'vague', 'images', 'formed', '2-dimesnional', 'classified', 'entered', 'style=', 'y', 'confluence', 'translate', 'politics', 'turns', 'golf', 'simulations', 'stochastic', '6.10820061e-05', 'quite', 'reusable', 'seaborn', 'real-life', 'first', 'recompiled', 'v', 'grammatically', 'adds', 'features', 'reproducible', 'guidance', 'assist', 'understands', 'casts', 'environment', 'anns', 'addressed', 'basket', 'probabilities', 'algorithms.some', 'focal', 'excluding', 'solves', 'witter', 'consequence', 'game', '1.15178166e-10', 'x|c', 'assumed', 'mining', 'portion', 'target', 'comprise', 'upper', 'starts', 'build', 'captured', '-u', 'harmful', 'lack', 'relationship', 'torus', 'concept.day', 'travel', 'followed', 'descent', 'drive', 'kwh', 'work', 'stores', '–', 'test_size', 'accuracy_score', 'referred', '17', 'exclusively', 'element', 'discipline', 'test_loss', 'actions', 'higher-level', 'spaces', 'links', 'vary', 'transformation', 'field', 'skilled', 'found', 'statistically', 'plt.subplot', 'heavily', 'basic', 'offers', 'similarities', 'equation', 'aris', 'algorithmsto', 'possibility', 'stems', 'array', 'searched', 'convolutional', '5,5', 'thatautomatically', 'technologies', 'svc_model.fit', '1488043e-06', 'jquery', 'payments', 'candy', 'updates', 'risks', 'act', 'utilized', 'housing_price', 'benefit', 'radars', 'whether', 'tea', 'running', 'desirability', 'dream', 'differentiate', 'non-facial', 'redundant', 'involve', 'reads', 'ogrammingthe', 'lockheed', 'paths', 'idea', 'border', 'multimodal', '0.8241', 'edicted_label', 'goes', 'ability', 'according', 'plt.xticks', 'us', 'β0', 'self-', 'resulting', '-', 'inferences', 'vehicles', 'multi-paradigm', 'perception', 'normaliz', 'constitute', 'breed', 'declared', 'automation', 'kneighbors', 'programmed', 'compensated', 'bag', 'garment', 'establish', 'siri', 'stationary', 'int', 'exp', 'wpf', 'result', 'measured', 'beginners', 'scientific', 'popular', 'vi', 'sort', 'sources', 'fail', 'arrays', 'examining', 'problem-parameter', 'spam', 'owned', 'one-hot', 'quantities', 'queen', 'bit', '1960', 'indications', 'hurdle', 'understand', 'trick', 'region', 'prevent', 'independent', 'poorer', 'background', 'svc_model', '1995', 'anticipating', 'lasso', 'experts', \"hue='opportunity\", 'ten', 'possible', 'yelp', 'box-cox', 'dramatically', 'evident', 'option', 'curse', 'subgroup', 'sequence', 'ozone', 'librar', 'grows', 'prerequisites', 'tagged', 'programmer', 'english', 'name', 'completed', 'unbelievable', 'focuses', 'angle', 'happening', 'graphical', 'described', 'sklearn.metrics', 'successful', 'targeted', 'get', 'b1', 'subjects', '8.0366623e-01', 'activation', 'hawkes', 'specifically', 'yields', 'from', 'enters', 'consent', 'withdrawing', 'challenges', 'purchased', 'incomplete', 'lean', 'nvidia', 'stack', 'bo', 'utilize', 'satisfactory', 'ith', \"'red\", 'vicinity', 'behind', '8.30939484e-07', 'desired', 'remark', 'matplotlib', 'boils', 'senders', 'mine', 'computationally', 'one', 'transactions', 'trial', 'precedence', 'items', 'structure', 'happens', 'kid', 'accepted', 'representations', 'classic', 'jvms', 'mutation', 'liberal', 'update', 'later', '1.36480646e-10', 'sequential', 'network', 'manageable', 'egression', 'walmart', 'test_images.shape', '#', '255', 'unorganized', 'respects', 'autonomy', 'trainings', 'variety', 'contain', 'solutions', 'plurality', 'could', 'entries', 'value.noisy', 'os', 'satellites', 'defaulting', 'frequencies', 'resolve', 'relay', 'instructions', 'β', 'pandas', 'measure', 'busing', 'levels', 'jersey', 'infrastructure', '.step', 'conjunction', 'transport', 'pace', 'los', 'api', 'root', 'gender', 'preliminary', 'distorted', 'f', 'remarkably', 'thelanguage', 'heart', 'fraction', 'acknowledge', 'natural', 'trained', 'vantage', 'interval', 'selective', 'keras.layers.dense', 'centuries', 'sites', 'co', 'requirements', 'rather', 'namely', 'adaptation', 'np.argmax', 'our', 'python', 'complex', 'left', 'instances', 'amount', 'well-', '“', 'satellite', 'methodssuch', 'vital', 'detect', 'both', 'y.', 'david', 'surveying', 'expected', 'on', 'develop', 'ant', 'humongous', 'live', 'success', 'simply', '5', 'arranged', 'c-sharp', 'x1', 'succeeding', 'fundamental', 'e-commerce', 'factoring', 'copy', 'outcome', 'areas', 'displaying', 'quote', 'increased', 'willing', 'noisy', 'multi-agent', 'experience', 'leaner', 'mimics', 'proportion', 'dimension', 'plt.figure', 'digital', 'expenses', 'prediction_result', 'city', 'multi', 'β2x2', 'dataanalysis', '0.86', 'lagoon', 'sixth', 'flooded', 'delhi', 'propagation', 'terminal', 'space', '75us/sample', 'justify', 'practically', 'essential', 'non-parametric', 'routines', 'actually', 'a', 'physical', 'team', 'hierarchy', 'ecosystems', 'roulette', 'by', 'modularity', 'directly', 'generally', 'agency', 'user-friendly', 'paramount', 'learned', 'set.data', 'industries', 'functioning', 'suggest', 'k-neighbors', 'convenient', 'dealt', 'prowl', 'specify', '1992', 'succession', 'characteristic', 'artificial', 'inconsistent', 'solid', 'concluded', 'google', 'explanations', 'value.', 'interpreted', 'tar', 'population', 'delayed', 'suggests', 'sq_foot', 'stl', 'lossless', 'streams', '02', 'consult', 'following', 'newly', 'black', 'endless', 'third', 'situation', 'xamarin', 'itselfthrough', 'checkpoint', 'utility', 'providing', 'wora', 'clear', 'compromised', 'square', 'years', 'perl', 'infiltration', 'frameworks', 'indicated', 'tpu', 'characters', 'changing', 'notice', 'finished', 'ones', 'entity', 'significance', 'transforms', 'train_test_spli', 'compilers', 'min', 'vector', 'set.', 'df', 'operating', 'component', \"/input/creditcard.csv'\", 'adverse', 'countergoals', 'across', 'decrease', 'a_fn-usec_-sales-win-loss.csv', 'strongly', 'blogs', 'quantitative', 'thus', 'batch', 'campaigns', 'cells', 'think', 'assess', 'finite', 'projects', 'amounts', 'restricted', 'offering', 'consist', 'individually', 'cyber-duck', 'ree', 'resolving', 'condition', 'cluster', 'windy', 'tenure', 'contribute', 'repetitive', 'processes', 'favorable', 'vectorized', 'agent', 'unmeasurable', '0x7f65fb64b5c0', 'return', 'if', 'isthe', 'exer', 'engaging', 'conda', 'difficulty', '0.77781', 'advantages', 'peer', 'learn', 'focusing', 'check', 'tfidf', 'tuple', 'compiling', 'speaks', 'additioncommand', 'synonyms', 'intuition', 'predicts', 'intuitive', 'hierarchical', 'supplies', 'questions', 'means', 'browser', 'optimum', 'active', 'sure', 'webpages', 'comprehensive', '0.17.0', 'signal', 'pd', 'farming', 'representation', 'selectively', 'computations', 'instructs', 'model.ensemble', 'descriptive', 'reached', 'failing', 'multinomial', 'ways', 'unable', 'sklearn', 'among', 'repository', 'discovering', 'represented', 'smarter', '0.11', 'rooms', 'nonlinear', 'pairs', 'collect', 'require', 'formats', 'elucidation', 'theory', 'debate', 'purposes', 'organisms', 'protocol', 'labels', 'url', 'humans', 'cross', 'review', 'problems', 'flown', 'engineered', 'reasons', 'user-', 'alternatives', '4.1718483e-08', 'human', 'peak', 'b0', 'deployment', 'evaluations', 'better', 'capacity', 'maximization', 'made', 'contains', 'sales_data.head', 'ever', 'futile', 'portability', 'taking', \"'test\", 'transforming', 'availability', 'overcome', 'recent', 'ibm', 'ratio', 'y=', 'feelings', 'rain', \"ed'\", 'assignment', 'ofdifferent', 'level', 'machine', 'elucidate', 'actual', 'objects', 'containing', 'forecast', 'people', 'prejudice', 'chapter', 'assume', 'futuristic', 'kinds', 'acquire', 'categories', 'runtime', 'effects', 'reality', 'analysts', 'requires', 'service', 'thought', 'like', 'nearly', 'make', 'tokyo', 'reliability', 'django', 'perform', 'explored', 'ge', '2003', 'predictors', 'inability', 'fraud', 'lessons', '+', 'logical', 'colors', 'withtensorflow', 'occurring', 'fundamentalml', 'strategy', 'mobile', 'reveal', 'labelencoder', 'ganism', 'required', 'quizanswer', 'ordinal', 'technique.python', 'forest', 'maintain', 'everyday', 'col', '1983', 'much', 'img', 'bedrooms', 'keywords,32', 'standardized', 'installing', \"'opportunity\", 'smoothened', 'breakthrough', 'communicating', 'via', 'as', 'funnel', 'matter', 'unseen', 'warranties', 'color', 'period', 'refined', 'resulted', 'interpretative', 'stands', 'events', 'time', 'problem', 'missing', 'leads', 'measurements', 'crash', 'panda', 'rely', 'converting', 'column', 'tries', 'failed', 'intricate', 'global', 'even', 'drawing', '0.51', 'plotting', 'capture', 'executing', 'enhanced', 'everyone', 'define', 'polynomial', 'hypothesize', 'source', 'influenced', 'set.now', 'fitter', 'end-to-end', 'similarity', 'line', 'importance', 'efficiently', 'rating', 'transformations', 'pipeline', 'edictions_array', 'responsible', 'centralized', 'i+1', 'plt.show', 'employ', 'assuming', 'date', 'elements', 'consistent', 'plt.yticks', 'until', 'function—', 'expectation', 'loop', 'effective', 'finish', 'engine', 'finance', 'mckinsey', 'bottom', 'feed', 'apriori', 'perceived', 'straight', 'ipython', 'transfer', 'cybersecurity', 'explore', 'district', '.set_color', 'proportionally', 'modelled', 'generations', 'revenue', 'find', 'normalize', '*', 'pred', '7', 'imagine', 'handwritten', 'representing', 'built', 'logistic', '28x28', 'command', 'mechanisms', 'budget', 'recommended', 'constitutes', \"result'cols\", 'facilitator', 'lower', 'appropriately', 'pre-', 'acknowledgment', 'emergence', 'of', 'object', 'hope', 'best', 'place', 'neigh', 'modules', 'ferenceencoder', 'ice', 'inverse', 'association', 'eggs', 'x-axis', 'academic', 'combination', 'algorithmreview', 'multi-', 'conversion', 'section', 'languages', 'judging', 'thisplot', 'binomial', 'decision', 'promising', 'though', 'information', 'tuning', 'trickthe', 'model.compile', 'program', 'βpxp', 'recognizes', 'temperature', 'publication', 'outcomes', 'generating', 'true_label', 'nltk', 'third-party', 'important', 'combines', 'accounts', '100', 'initiated', 'n_neighbors=3', 'fuzzy', 'real', 'precision', 'presence', 'always', 'rule-based', 'biggest', 'combining', 'sensitive', 'model-based', '+b', 'also', 'fire-fighting', 'assignments', 'surrounds', 'communities', 'neighboring', 'categories-1', 'available', 'believe', 'processors', 'variable', 'route', 'without', 'reduces', 'depending', 'approximations', 'beused', 'age', 'amsterdam', 'uncertainty', 'weather', 'webpage', 'tasked', 'members', 'scipy', 'jvm', 'measurement', 'intermediate', 'lists', 'applicable', 'bags', 'cm', 'grandmother', 'slope', 'occurs', 'located', 'secondary', 'easier', 'skill', 'represents', 'types', 'ankle', 'diligently', 'generation', 'ignoring', 'winner', 'encode', 'expand', 'operate', 'adjectives', 'diversity', 'irtual', '0.8643', 'num_rooms', 'k-1', 'real-time', 'manufacturing', '0-20', 'down-sampling', 'sklearn.decomposition', 'fantasy', 'parents', 'male|height=150', 'path', 'needed.categorical', 'adjusted', 'amazon.com', 'smoothen', \"''\", 'minecraft', 'wavelet', 'epochs', 'dtypes', 'integration', 'reward', 't-', 'incur', 'enormity', 'neural', 'money', 'merge', 'example', 'class_names', 'reviewed', 'making', 'layers.the', 'printed', 'achievable', 'capabilities', 'pertinent', 'to', 'pd.read', 'here', 'stating', 'unlabeled', '5+7', 'plot', 'versatile', 'editors', 'facial', 'subjectively', 'master', 'months', 'began', 'characteristics', 'obtain', 'limitation', '2000', 'gaming', 'automated', 'widely', 'sequences', 'pre-trained', 'bred', 'constraints', 'hundreds', 'university', 'tend', 'axes3d', 'leverages', 'medical', 'unusual', 'dimensionality', 'product', 'singular', 'falls', 'wire', 'role', 'row0', 'distinctions', 'database', 'inform', 'key1', 'post', 'key', 'mapping', 'powerful', 'arguments', 'if-then', 'interpret', 'cohen', '..', 'developed', 'returns', 'euler', 'maximizing', 'observation', 'forthe', '35', 'grab', 'reflected', 'nodejs', 'radiologist', 'family', 'fairly', 'jupyter', 'uncover', 'scaling', 'rigidly', 'that', 'paris', 'schooling', 'percentage', 'trend', 'context', 'screen', 'female', '//community.watsonanalytics.com/wp', 'increasingly', 'titled', 'prerequisite', 'off-the-', 'f1_score', 'arthur', 'restrict', 'detecting', 'see', 'platforms', 'n=2', 'pertaining', 'fall', 'cars', 'position', 'substituted', 'k', 'dif', '?', 'normalization', 'documents', 'horizon', '.day', '—these', 'maximizes', 'cardinality', 'tower', 'going', 'reduced', 'overlooked', 'converge', 'while', 'declarative', 'automotive', 'threats', '0.5018', '1974', 'share', 'meaningless', 'anyone', 'truth', 'disconnected', 'slightly', 'general-purpose', 'sneakers', 'units', 'flavors', 'typically', 'spectrum', 'starting', 'supply', 'producing', 'migration', 'miscellaneous', 'far', 'product.ensemble', 'retailer', 'exploration', 'divide', 'sufficiently', 'status', 'lives', 'whatever', '8.03666413e-01', 'c', 'object-oriented', 'vision', 'few-shot', 'mixing', 'evolved', 'visually', 'achieved', 'purchase', 'fixes', 'buyer', 'statistics', 'humid', 'implementing', 'databases', 'concept', 'aspect', 'xtrain', 'replace', 'properties', 'blouses', 'continuous', 'machinelearning', 'gradually', 'statistic', 'phase', 'kick', 'scale', 'conflicts', 'event', 'self-contained', 'picture', 'denied', 'students', '+e^', 'past', 'fixed', 'lattice', 'get_train', 'with', 'losing', 'activities', 'yellowbrick', 'import', 'constant', 'enjoy', 'conversations', 'obtained', 'tail', 'traditional', 'tf.keras', 'rendering', 'point', 'testing', 'sorted', 'accessible', 'others', 'coefficients', 'volumes', 'way', 'common', 'n', '”', 'fulfill', 'weights', '3/5', 'incurring', 'communicate', 'coin', 'phrase', 'involved', 'n-dimensional', 'improved', 'pool', 'auto-complete', 'o/p', 'advanced', 'payment', 'contributing', 'notebook', 'interpretation', 'missed', 'multi-dimensional', 'plenty', 'letter', 'microsoft', 'case', '.in', 'pr', 'acquired', 'automobile', 'body', 'creative', 'volume', 'ferent', '3', 'server', 'simpler', 'foster', 'inferenceday', 'self-learn', 'segmented', 'sharing', 'researcher', 'labeled', 'pictures', 'query', 'max', 'plt', 'gauge', 'algebra', 'show', 'light', 'identical', 'market', 'applies', 'conditioning', 'significantly', 'mimic', 'iterative', 'densely', 'thelikelihood', 'positive', 'fewer', 'tthews_corrcoef', 'identifier', 'local', 'subsequent', 'coders', 'conventionally', 'episode', 'umbrella', 'predators', 'meters', 'yielding', 'rules-based', 'simulate', 'completely', 'stop', 'visualization', 'the', 'nouns', 'pay', 'softmax', 'test', 'majority', 'ms', 'librarie', 'subset.to', 'eventually', 'shopping', 'plt.bar', 'negatives', 'iteration', 'trousers', 'cross-platform', 'probably', '71us/sample', 'grouping', 'requesting', 'nitrate', 'depicted', 'belong', 'bars', '8.3093937e-07', 'attackers', 'patterns', 'reducible', 'enter', 'flow', 'carried', 'k-', 'play', 'vehicle', 'case-based', 'employing', 'pheromones', 'indicating', 'any', 'tweaks', 'learner', 'cmd', '0.8671', '-100', 'high-dimensional', 'purpose', 'naïve', 'college', 'male', 'under', 'valuable', 'games', 'split_mean', 'transition', 'collected', 'regressions', 'flourish', '9', '0.3382', 'fitness', 'limit', 'academia', 'configure', 'problem-solving', 'interactions', 'sequentially', 'oject-based', 'widespread', 'words', 'rest', 'modification', 'bayes', 'hence', 'incorrect', 'hyper', 'in-', 'elu', 'maximum', 'explained', 'couple', 'certain', 'deals', '30', 'stacking', 'iphones', 'directory', 'texture', 'color=color', 'minus', 'shared', 'although', 'extract', 'built-in', 'envision', 'e', 'c|x', 'map', 'proven', 'cherished', 'academy', 'improper', 'e^', '20', 'quasi-newton', 'boundaries', 'revolution', 'raspberry', 'recap', 'cyber', 'sample', 'house', 'estimations', 'another', 'windows', 'converted', 'modelling', 'unix', 'laboratories', 'transformed', 'retrieved', 'corpus', 'category/class', '9-d', 'follows', 'address', 'upward', '6.58371528e-06', 'installation', 'draw', 'generic', 'life', 'non-linear', 'either', 'select', 'etc', 'proposed', '0.2967', 'alternatively', 'regarded', 'ai', \"'bag\", 'innovative', 'medium', 'apple', 'implemented', 'accurately', 'csv', 'optimizer', 'consequences', 'b1=', '1.1517859e-10', 'concurrently', 'multi-variable', 'efficiency', 'hue', 'visualized', 'paradigm', 'researchers', 'unit', 'faces', 'misinterpretation', 'tame', 'feeding', 'realistic', 'propose', 'explicit', 'inherent', 'multilayered', 'implement', 'sklearn.svm', 'confuse', 'question', 'customer', 'ideal', 'scientists', 'scalable', 'applications', 'anywhere', 'robert', 'convey', 'low', 'sklearn.model_selection', 'intelligence', 'dividing', 'done', 'client', 'assign', 'demonstrates', 'damage', 'version', 'enhance', 'amend', 'women', 'avoid', 'messaging', 'wonder', 'components', 'narratives', 'pairwise', 'iv', 'owing', 'plant', 'lambda', 'fs', 'suitable', 'british', 'concurrency', 'systems', 'but', 'infrastructures', 'gathered', 'sets', 'encoded_value', 'efficacy', 'prevalent', 'previously', 'wheels', 'workflow', 'outputs', 'monetary', 'tech', 'history', 'cumbersome', 'goal', 'taxing', 'public', 'fire', 'rewarding', 'java', 'train_labels', 'unknown', 'k=', 'evaluated', 'https', 'friendly', 'classification/clustering', 'seeing', '86', 'recommendation', 'automate', 'loss', \"'shirt\", 'comprehend', 'licensed', 'irrigated', 'boot', 'treated', 'plug', 'tuples', 'became', 'library', 'airline', 'scrolling', 'typing', 'hydraulic', 'hyper-plane', 'y-intercept', 'merging', 'multi-class', 'paraphrase', 'learning', 'bayesian', 'coordination', 'read_csv', 'regional', 'operators', 'polls', 'ca', 'col2', 'prospective', 'statistical', 'inspired', 'image', '.java', 'surface', 'gas', '10000/10000', 'andkeywords', 'staff', 'learningallows', 'locations', 'copyright', 'produces', 'established', 'customize', '12/25', 'roku', '1004785', 'access', 'keep', 'downloadable', 'collection', 'specifying', 'lines', 'sns', 'indivisible', '10000', 'evolving', 'competitive', 'fact', 'evolution', 'interests', 'determining', ':2.0f', 'generalizing', 'crafted', '<', 'over-', 'serve', 'future', 'behavior', 'offset=10', 'dtype=uint8', 'notepad', 'cryptocurrency', 'select-act', 'difference', 'flash', 'becoming', 'not', 'may', 'practice', 'trip', 'differentiating', 'analysis', 'notation', 'intelligent', 'carefully', '6.1082108e-05', 'strategies', 'file', 'schools', 'follow', 'inlasso', 'variables', 'office', 'sentiment', 'comprised', 'set.transfer', 'read', 'silicon', 'reason', 'ann', 'explain', 'svms', 'tested', 'cooperation', 'ordered', 'hypothesis', 'depleting', 'bias', 'sophisticated', 'closer', 'unauthorized', '___', 'mutually', 'operator', 'word', 'describe', 'log', 'expertise', 'repeating', 'highly', 'learningalgorithms', 'tops', 'employed', 'basis', 'number', '75', 'ensemble', 'underlying', 'inches', '©', 'expands', 'typed', 'kqml', 'regression', 'pre-processingby', 'dimensions', 'identifying', 'seeks', 'now', 'epochs=5', 'imprecise', '0.2', 'causal', 'establishing', 'bear', 'objectively', 'towns', 'libraries', 'permission', 'oduction', 'tocomputer', 'finally', 'write', 'advances', 'tips', 'pd.read_csv', 'selection', 'achieve', 'individual', 'currency', 'practitioners', 'often', 'mathematical', 'periodic', 'microchips', 'mpl_toolkits.mplot3d', 'focus', 'ranging', 'sales_win_loss', 'necessary', 'surroundings', 'reconstruction', 'fitting', 'matrices', 'compressed', 'high-end', '‘', '4/5', 'type', \"'\", 'shrinkage', 'revealing', 'addresses', 'score', 'resource', \"'ll\", 'block', 'specificity', 'resembling', 'default', 'particularly', 'order', 'priceless', 'predictions_single', 'manipulation', 'filter', 'contrast', 'fortunately', '0.3653', 'listed', 'useful', 'iii', 'detailed', 'tool', 'mac', 'only', 'consumer', 'occurrence', 'passed', 'paying', 'account', 'discuss', 'drugs', 'sales', 'linearsvc', 'industry', 'advent', 'consider', 'coordinates', 'look', 'amateurs', 'jobli', 'row1', 'stated', 'altering', 'diagram', 'algorithms.to', 'step', 'borrowing', 'data_train', 'called', 'enable', 'argument', '1989', 'leaf', 'offspring', 'rangelands', 'allowing', 'plt.xlabel', 'function', 'firms', 'cises', 'dependency', 'viewed', 'minimized', 'codes=', 'equilibrium', 'big', 'discover', 'studied', 'control', 'assurance', 'pertains', 'cyclic', 'foundational', 'evaluating', 'retrieve', 'thatneeds', 'entities', 'suggesting', 'results', 'ga', 'manage', 'attributes', 'website', 'random', 'healthcare', 'edge', 'negotiating', 'business', 'consideration', 'varying', 'extreme', 'winamp', 'dot', 'along', 'objective', 'candies', 'attempt', 'unlike', 'popularity', 'system/370', 'reuse', 'huge', 'understanding', 'achievement', 'factors', 'career', 'outshine', 'setsdata_train', 'less', 'safeguard', 'expose', 'tricks', 'approximation', 'details', 'dense', 'head', 'data=', '.dimensionality', 'term', 'i/o', 'traffic', 'its', 'embedding', 'test_labels', 'gatesa', 'likelihood', 'table', 'compare', 'tensorflow', 'classify', 'neurons', 'describing', 'crossover', 'verbs', \"'t-shirt/top\", 'schnauzer', 'fully', 'designs', 'dogs', 'area', 'binary', 'quicker', 'refrigerators', 'finding', 'pre-processing', 'fuzzified', 'shield', 'plt.imshow', 'explicitly', 'heating.dimensionality', 'failure', 'flour', 'direction', 'directives', 'stock', 'atmosphere', 'closest', 'recurring', 'nlp', 'worl', 'fits', 'allocated', 'man', 'managing', 'flexible', 'ensure', 'free', 'precisely', 'searches', 'generated', 'sun', 'training', 'because', 'autocatalytic', 'upon', 'linux', 'shuffling', 'parametric', 'design', 'conclusions', 'sub-network', 'reasoning', 'introduction', 'class', 'settings', 'prescriptive', 'significant', 'telecom', 'go', 'low-level', 'validation', 'le.fit_transform', 'analytical', 'through', 'transform', 'plan', 'cart', 'handed', 'unidirectional', 'case-by-case', 'turn', 'taste', 'chip', 'framework', 'cas', 'ii', 'populations', 'developers', 'learns', 'formal', 'processed', 'variations', 'neigh.predict', 'expenditure', 'mass', 'residence', 'mutating', 'and', 'module', 'onehotencoder', 'arises', 'relate', 'fundamentals', 'applicator', 'linked', 'k-gaussian', 'comparative', 'self-driving', 'procedures', 'options', 'effect', 'strong', 'pixels', 'target_train', 'relevant', 'deterministic', 'gaussian', 'influence', 'true/false', 'spreadsheet', 'cols', 'therefore', 'encryption', 'main', 'osofts', \"line's\", 'outperformed', 'teaches', 'activation=tf.nn.softmax', 'research-based', 'code', 'achieving', 'considered', '60000/60000', 'maximum-likelihood', 'pre-existing', 'classifications', 'measures', 'continued', 'products', 'wish', 'implied', 'speed', 'weaknesses', 'graphs', 'response', 'categorized', 'extraction', 'syntax', 'week', 'move', 'performances', 'need', 'w', 'ef', 'trees', 'points', 'side', 'analyzed', 'works', 'areavailable', 'highest', 'fictitious', 'reducing', 'waste-water', 'relates', 'click', 'varied', 'classifier', 'regular', 'varieties', 'alphabetical', 'aims', 'generate', 'group', 'print', 'balanced', 'able', 'pso', 'android', 'once', 'relational', 'procedure', 'helps', '10,10', 'closeness', 'methods', 'digit', 'impenetrable', 'chapters', 'initial', 'developer', 'load', 'server-side', 'considers', 'beauty', 'precept', 'underfitting', 'vii', 'defining', 'contributed', 'renders', 'technical', 'social', 'bidirectional', 'technology', 'pete', '150', 'unfettered', 'creating', 'medians', 'attempts', 'off-lattice', 'generalization', 'understood', 'short', 'system', 'scoring', 'train_images.shape', 'planning', 'attacks', 'ensorflow', 'least', 'fraudulent', 'final', 'prevention', 'genetic', 'numerous', 'logic', 'crime', 'present', 'xtest', '80', 'circumstances', 'maneuvers', 'replacement', 'portray', 'category_encoders', 'things', 'wise', 'covariance', 'sell', 'fill', 'allows', 'subroutine', 'quick', 'hyper-planes', 'untilthey', 'known', 'xml', 'examine', 'angeles', 'sneak', 'various', 'easily', 'nominal', 'facts', 'sne', 'given', 'resolution', '>', 't-sne', 'exported', 'appealing', 'this', 'different', \"'re\", 'assets', 'sns.set', 'title', 'we', 'ganizations', 'nation', 'execute', 'grouped', 'occurred', 'minutes', '11.7,8.27', 'purchases', 'metrics=', 'choose', 'feedback', 'ranges', 'acts', 'existence', 'color_codes=t', 'model.predict', 'seamless', '==', 'package', 'changed', 'undergoing', 'additions', 'incorporating', 'standard', 'range', 'boolean', 'incapable', 'glmnet', 'resources', 'integrate', 'limited', 'refit', 'highest-level', 'orientation', '===================', 'existing', 'complementary', 'protect', 'giving', 'defined', 'at', 'essentially', '2007', 'pdf', 'hidden', '.classification', 'little', 'new', 'smart', 'colonies', 'shortest', 'day', 'regressors', 'usage', 'labeling', 'selling', 'tasks', 'dashboards', 'encoding', 'symposium', 'solution', 'denotes', 'red', 'when', 'row', 'exercises', 'track', 'maintenance', 'standards', 'milestones', 'inaccuracies', 'accomplish', 'o', 'store', 'based', 'sensing', 'acompany', 'scenario', 'academycopyright', 'authorized', 'clusters', 'software', 'forecasting', 'shirts', 'driven', 'trim=t', 'assemblers', 'scores', 'factor', 'coding', 'apart', 'reserved', 'summarizing', 'correct', 'walking', 'gathering', 'delimiter', 'len', 'html', 'industrial', 'intercept', 'labs', 'commodity', 'accuracy_scor', '0.814550580998', 'memorize', 'two-class', 'mnist', 'depicting', 'hand', 'gpu', 'bridge', 'deploy', 'creation', 'electronics', 'project-based', 'negative', 'dependent', 'businesses', 'realities', \"state'\", 'full', 'scheduling', 'performed', 'posterior', '=', 'contemporary', 'throttle', 'would', 'bell', '6.5837266e-06', 'recommend', 'regarding', 'approaches', 'decisions', 'trainingdata', 'optimize', 'distributed', 'identified', 'exploratory', 'process', 'breeds', 'sense', 'really', 'decomposition', 'exercise', 'recollection', 'dictate', 'unstructured', 'keras.datasets.fashion_mnist', 'educational', 'commonly', 'rise', 'pronounced', 'carrying', 'hot', 'inference', 'data_test', 'sections', 'structures', 'preprocessing', 'scholarly', 'jump', 'fruit', '80:20', 'per', 'depict', 'mistakes', 'assistance', 'samuel', 'bernoulli', 'np.random', '!', 'sampling', 'absolute', 'including', 'importing', 'cbr', 'segmentation', 'air', 'mediators', 'trail', 'msterdam', 'integral', 'disease', 'pedestrians', 'encountered', 'electronic', 'for', 'correctly', 'drowning', 'internal', 'gain', 'datasets', 'equipped', 'ios', 'adobe', 'applied', 'architecture', 'accurately.', 'fifth', 'app', 'pi', 'figures', 'manmade', 'i', 'scaled', 'masculine', 'content', 'executed', 'messages', 'upsurge', 'chi-square', 'informs', 'they', 'image-related', '==============================', 'self-train', 'scalar', 'associate', 'calibration', 'lattices', 'mozilla', 'defective', 'govern', 'selected', 'steady-state', 'predictions', 'whole', 'top', 'optimization', 'you', 'cakes', 'unchained', '1.49914682e-01', 'dog', 'edges', 'accounted', 'presented', 'clothing', 'marching', 'transferring', 'backwarddif', 'coupe', 'location', 'deemed', 'manner', 'multidimensional', 'dummy', 'figsize=', '&', 'input_shape=', 'managed', 'forward', 'tf.estimator', 'skills', 'organized', '1', 'crux', 'parse', 'img,0', 'weighted', 'oftraining', 'list', 'impacted', 'caused', 'occurrences', 'citizens', 'state-of-the-art', 'apis', 'smooth', 'what', 'insights', 'activated', 'setting', 'categorical', 'demographics', 'sales_data', 'experiencing', 'retain', 'sympy', 'internet', 'acl', 'analyze', 'minimization', 'encoded', 'person', 'robotics', 'dataset', 'structure-oriented', 'commands', 'observe', 'services', 'grace', 'king', 'easy', 'cooking', 'informationto', 'determination', 'non-linearity', 'landscape', 'model', 'solidify', 'index', 'topic', 'simplest', 'video', 'immediate', 'choice', 'ofgenetic', 'straightforwardly', 'likewise', 'dummy/indicator', 'ants', 'or', '2/5', 'indication', 'represent', 'deformed', 'encompasses', 'readers', 'days', 'vendor', '3.1148918e-06', 'yield', 'lifeexamples', 'includes', 'cubes', 'filling', 'system.3', 'importconfusion_matrix', 'sensors', 'task', 'studies', 'k-means', 'st', 'rescue', 'beyond', 'n/a', 'help', 'nature', '0.30', 'just', 'did', 'base', 'attenuate', 'hurricanes', 'repeated', \"model's\", 'interact', 'every', '10,000', 'measurable', 'organizations', 'signatures', 'nearest', 'foundation', '4.63472381e-', 'generator', 'plt.style.use', 'filtered', 'datalogy', '0.18.0', 'principle', 'lifecycle', 'elects', 'lieu', 'beneficial', 'synchronously', 'boundary', 'potentially', 'urls', 'hours', 'ml', 'sample.label', 'neigh.fit', 'organize', 'chosen', 'chromosomes', 'numpy', 'digits', 'promotes', 'cohesive', 'omissions', 'instead', 'necessity', 'keras.layers.flatten', '8.27px', 'end', 'estimated', 'smaller', 'automatically', 'high', 'opportunity', 'margin', 'projected', 'records', 'iteratively', 'forecasts', 'amounting', 'rme', 'property', 'imaging', 'files', 'historical', 'detection', 'auxiliary', 'fishermen', 'string', '-5', 'pleased', 'next', 'recognize', 'issues', 'scheme', '2-d', 'semi-supervised', 'which', 'autonomous', 'algebraic', 'drop', 'recommendations', '1.0', 'right', 'dealing', 'positions', 'labelbinarizer', 'embedded', 'parts', 'installed', 'frequency', 'rapidly', 'assumptions', 'informative', 'mainly', 'entirely', 'credited', 'relying', 'develops', \"'woman\", '-1.0', 'download', 'stages', 'facing', 'equivalent', 'patient', 'plt.grid', 'latest', 'tree', 'cell', 'depicts', 'gadgets', 'computational', 'inspiration', 'shelf', 'utilizes', 'enough', 'let', 'efficient', 'sugar', 'change', 'these', 'simplifying', 'center', 'unordered', '60000', 'reactions', 't+1', 'advisor', 'there', 'advisable', 'progressively', 'playing', 'throughout', 'discriminator', 'freely', 'construct', 'algorithm', 'unlikely', 'parallel', 'findings', 'thoughts', 'col1', 'back', 'deriving', 'etl', 'offer', 'maximize', 'infinite', 'customized', '.quick', 'le', 'enjoyed', 'front-end', 'convert', 'drilling', 'scikit-learn.org', 'stored', 'disclaimer', 'transferred', 'leverage', 'data', 'capacities', 'hardware-driven', 'performing', 'usually', \"'ve\", 'disadvantage', 'dtype=float32', 'oracle', 'processingday', 'pl', 'conduct', 'derived', 'assembly', 'earliest', 'tweetmap', 'manual', 'balance', 'comparisons', 'terminologies', 'output', 'oceans', 'environment.random', 'most', 'expanded', 'lead', 'nuances', 'converts', 'domains', 'experiments', 'spatial', 'predictive', 'media', 'filtering', 'inferential', 'two', 'evaluate', 'consolidate', '.3', 'executions', 'directed', 'signaling', 'liner', 'initially', 'over', 'activation=tf.nn.r', '10', 'cleanest', '1/d', 'decimals', 'cases', 'straightforward', 'scenarios', 'total', 'conclusion', 'unsuitable', 'prediction', 'consumption', 'dischar', 'misinterpret', 'manipulate', 'diagrams', 'put', 'opportunities', 'size', 'recently', '1s', 'predictor', 'distance', 'incorporated', 'constituting', 'receiving', 'exclusive', 'provide', 'javascript', 'organization', 'exist', 'companies', 'focused', 'heating', 'preceding', 'kneighborsclassifie', 'encoder', 'calculating', 'unclassified', 'ultimately', '0.8846', 'weight', 'overturned', 'best-', 'vs', 'reusability', 'discovered', 'completing', 'infinity', 'navigation', 'document', 'datase', 'detail', 'servers', 'behaviors', 'memory', 'models', 'one-to-one', 'fashion_mnist.load_data', 'set', 'serves', 'added', 'tank', 'cosine', 'it', 'dominant', 'ocessing', 'e^-value', '65', 'amazon', 'storage', 'elasticnet', 'concerted', 'uses', 'hold', \"optimizer='adam\", 'acc', 'classifiers', 'reputed', 'instance-based', 'neighbors', 'indicates', 'come', 'generates', 'downloading', 'vectors', 'hands-on', 'recipients', 'oops', 'revise', 'alexa', 'modern', 'sub-', 'cycle', 'warming', 'become', 'color=', 'zippers', 'investments', 'ofuncertainty', 'boasts', 'e.g', 'steps', 'feature', 'metrics', 'viewing', 'employs', 'tell', 'intrusion', '40-60', 'distribute', 'seamlessly', 'mellon', 'plt.ylim', 'compact', 'suite', 'consumers', 'distinctive', '1972', 'greater', 'box', '.predict', 'weighting', 'execution', 'proficiency', 'calculated', 'shape', 'bad', 'beimplemented', 'policy', '1.4991476e-01', 'deeper', 'shadow', 'laborious', 'rue', 'cities', 'sentence', 'catastrophes', 'compiler', 'install', 'y=e^', 'automatic', 'alphanumeric', 'numbers', 'tfm', 'earlier', 'multiple', 'proper', 'taken', 'jeans', 'data=sales_data', 'weapon', '1.3648087e-10', 'leveraged', 'near', 'dist_transport', 'interpreters', 'sectors', 'machines', 'state', 'statisticians', 'fourth', '0.8897', 'hypotheses', 'worksheet', 'optimized', 'losses', 'firm', 'analytics', '.these', 'gene', 'reinforce', 'plotplt.show', 'representative', 'chinos', 'adequate', 'exactly', 'k-nn', 'high-level', 'home', 'cream', 'alone', 'similarly', 'b', 'useless', 'arithmetic', 'blackboard', 'correlations', 'approach', 'tracking', 'rows'}\n"
          ]
        }
      ],
      "source": [
        "keywords = set(keywords)\n",
        "print(keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "second-malawi",
      "metadata": {
        "id": "second-malawi",
        "outputId": "ef4889ce-7915-47a4-b3c2-d8c82e40fdbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "built\n"
          ]
        }
      ],
      "source": [
        "searchItm = 'Built'.lower()\n",
        "if searchItm in keywords:\n",
        "    print(searchItm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alone-trouble",
      "metadata": {
        "id": "alone-trouble",
        "outputId": "3aeeedd8-332d-479f-821e-d16320d2a247"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.18.13-cp38-cp38-manylinux2010_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 5.8 MB/s eta 0:00:01     |████████████████████████▍       | 4.9 MB 5.8 MB/s eta 0:00:01\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.18.13\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pymupdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "informational-particle",
      "metadata": {
        "id": "informational-particle",
        "outputId": "9d37312f-7dd8-4045-92c0-1744b4a1ac89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fitz in /usr/local/lib/python3.8/dist-packages (0.0.1.dev2)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.8/dist-packages (from fitz) (3.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from fitz) (1.6.2)\n",
            "Requirement already satisfied: httplib2 in /usr/lib/python3/dist-packages (from fitz) (0.14.0)\n",
            "Requirement already satisfied: nipype in /usr/local/lib/python3.8/dist-packages (from fitz) (1.6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fitz) (1.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fitz) (1.19.5)\n",
            "Requirement already satisfied: pyxnat in /usr/local/lib/python3.8/dist-packages (from fitz) (1.4)\n",
            "Requirement already satisfied: configobj in /usr/lib/python3/dist-packages (from fitz) (5.0.6)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.8/dist-packages (from fitz) (5.0.2)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.8/dist-packages (from nibabel->fitz) (20.9)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (1.4.2)\n",
            "Requirement already satisfied: rdflib>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (5.0.0)\n",
            "Requirement already satisfied: traits!=5.0,>=4.6 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (6.2.0)\n",
            "Requirement already satisfied: filelock>=3.0.0 in /root/.local/lib/python3.8/site-packages (from nipype->fitz) (3.0.12)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (2.5.1)\n",
            "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (7.1.2)\n",
            "Requirement already satisfied: prov>=1.5.2 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (2.0.0)\n",
            "Requirement already satisfied: etelemetry>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (0.2.2)\n",
            "Requirement already satisfied: simplejson>=3.8.0 in /usr/lib/python3/dist-packages (from nipype->fitz) (3.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.8/dist-packages (from nipype->fitz) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas->fitz) (2019.3)\n",
            "Requirement already satisfied: six>=1.15 in /usr/local/lib/python3.8/dist-packages (from pyxnat->fitz) (1.16.0)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.8/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.8/dist-packages (from pyxnat->fitz) (0.18.2)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.8/dist-packages (from pyxnat->fitz) (4.6.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from pyxnat->fitz) (2.25.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=14.3->nibabel->fitz) (2.4.7)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.8/dist-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/lib/python3/dist-packages (from networkx>=2.0->nipype->fitz) (4.4.2)\n",
            "Requirement already satisfied: ci-info>=0.2 in /usr/local/lib/python3.8/dist-packages (from etelemetry>=0.2.0->nipype->fitz) (0.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.20->pyxnat->fitz) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.20->pyxnat->fitz) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->pyxnat->fitz) (1.26.4)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/lib/python3/dist-packages (from requests>=2.20->pyxnat->fitz) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install fitz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "limiting-headquarters",
      "metadata": {
        "id": "limiting-headquarters",
        "outputId": "d7c4cf3a-6dd1-44e5-8d8d-9ce2292bfab4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: frontend in /usr/local/lib/python3.8/dist-packages (0.0.3)\r\n",
            "Requirement already satisfied: uvicorn>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from frontend) (0.13.4)\r\n",
            "Requirement already satisfied: starlette>=0.12.0 in /usr/local/lib/python3.8/dist-packages (from frontend) (0.14.2)\r\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.8/dist-packages (from frontend) (0.6.0)\r\n",
            "Requirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from frontend) (1.1.0)\r\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.8/dist-packages (from uvicorn>=0.7.1->frontend) (0.12.0)\r\n",
            "Requirement already satisfied: click==7.* in /usr/local/lib/python3.8/dist-packages (from uvicorn>=0.7.1->frontend) (7.1.2)\r\n"
          ]
        }
      ],
      "source": [
        "!pip3 install frontend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "maritime-lingerie",
      "metadata": {
        "id": "maritime-lingerie",
        "outputId": "76e7ef6e-2a59-4539-8946-6085a785b12c"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Directory 'static/' does not exist",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/mnt/d/JupyterNotebooks/Extract Text From PDF and search .ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://127.0.0.1:8080/mnt/d/JupyterNotebooks/Extract%20Text%20From%20PDF%20and%20search%20.ipynb#ch0000016vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfitz\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/fitz/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/fitz/__init__.py?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfrontend\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/fitz/__init__.py?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtools\u001b[39;00m\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/fitz/__init__.py?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpath\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mop\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/__init__.py?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mevents\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/__init__.py?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/__init__.py?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcomponents\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/__init__.py?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclipboard\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/__init__.py?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mevent_mixins\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/__init__.py?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhash_change\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/clipboard.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/clipboard.py?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mevent_mixins\u001b[39;00m \u001b[39mimport\u001b[39;00m ClipboardDataMixin\n\u001b[0;32m----> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/clipboard.py?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdom\u001b[39;00m \u001b[39mimport\u001b[39;00m Event\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/clipboard.py?line=3'>4</a>\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mClipboardEvent\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/clipboard.py?line=6'>7</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mClipboardEvent\u001b[39;00m(Event, ClipboardDataMixin):\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dom.py:439\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dom.py?line=434'>435</a>\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dom.py?line=435'>436</a>\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dom.py?line=438'>439</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatcher\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dispatcher.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dispatcher.py?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstarlette\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mendpoints\u001b[39;00m \u001b[39mimport\u001b[39;00m WebSocketEndpoint\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dispatcher.py?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstarlette\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwebsockets\u001b[39;00m \u001b[39mimport\u001b[39;00m WebSocket\n\u001b[0;32m---> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dispatcher.py?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m config, server\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dispatcher.py?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39masync_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m later_await\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dispatcher.py?line=17'>18</a>\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mreact\u001b[39m\u001b[39m'\u001b[39m]\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=20'>21</a>\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mroute\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrun\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=22'>23</a>\u001b[0m app: Any \u001b[39m=\u001b[39m Starlette(debug\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mDEBUG)\n\u001b[0;32m---> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=23'>24</a>\u001b[0m app\u001b[39m.\u001b[39mmount(config\u001b[39m.\u001b[39mSTATIC_ROUTE, StaticFiles(directory\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mSTATIC_DIRECTORY), name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mSTATIC_NAME)\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=24'>25</a>\u001b[0m app\u001b[39m.\u001b[39madd_middleware(GZipMiddleware)\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=25'>26</a>\u001b[0m app\u001b[39m.\u001b[39madd_middleware(\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=26'>27</a>\u001b[0m     CORSMiddleware,\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=27'>28</a>\u001b[0m     allow_origins\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=30'>31</a>\u001b[0m     allow_headers\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=31'>32</a>\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/starlette/staticfiles.py:55\u001b[0m, in \u001b[0;36mStaticFiles.__init__\u001b[0;34m(self, directory, packages, html, check_dir)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/starlette/staticfiles.py?line=52'>53</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_checked \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/starlette/staticfiles.py?line=53'>54</a>\u001b[0m \u001b[39mif\u001b[39;00m check_dir \u001b[39mand\u001b[39;00m directory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(directory):\n\u001b[0;32m---> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/starlette/staticfiles.py?line=54'>55</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDirectory \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Directory 'static/' does not exist"
          ]
        }
      ],
      "source": [
        "import fitz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ranging-flexibility",
      "metadata": {
        "id": "ranging-flexibility",
        "outputId": "68e891e3-f8b3-4388-af59-a1765c9ebb02"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Directory 'static/' does not exist",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/mnt/d/JupyterNotebooks/Extract Text From PDF and search .ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://127.0.0.1:8080/mnt/d/JupyterNotebooks/Extract%20Text%20From%20PDF%20and%20search%20.ipynb#ch0000017vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfitz\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://127.0.0.1:8080/mnt/d/JupyterNotebooks/Extract%20Text%20From%20PDF%20and%20search%20.ipynb#ch0000017vscode-remote?line=2'>3</a>\u001b[0m \u001b[39m### READ IN PDF\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://127.0.0.1:8080/mnt/d/JupyterNotebooks/Extract%20Text%20From%20PDF%20and%20search%20.ipynb#ch0000017vscode-remote?line=3'>4</a>\u001b[0m doc \u001b[39m=\u001b[39m fitz\u001b[39m.\u001b[39mopen(\u001b[39m\"\u001b[39m\u001b[39mMasterPython.pdf\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/fitz/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/fitz/__init__.py?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfrontend\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/fitz/__init__.py?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtools\u001b[39;00m\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/fitz/__init__.py?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpath\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mop\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/__init__.py?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mevents\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/__init__.py?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/__init__.py?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcomponents\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/__init__.py?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclipboard\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/__init__.py?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mevent_mixins\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/__init__.py?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhash_change\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/clipboard.py:2\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/clipboard.py?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mevent_mixins\u001b[39;00m \u001b[39mimport\u001b[39;00m ClipboardDataMixin\n\u001b[0;32m----> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/clipboard.py?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdom\u001b[39;00m \u001b[39mimport\u001b[39;00m Event\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/clipboard.py?line=3'>4</a>\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mClipboardEvent\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/events/clipboard.py?line=6'>7</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mClipboardEvent\u001b[39;00m(Event, ClipboardDataMixin):\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dom.py:439\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dom.py?line=434'>435</a>\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dom.py?line=435'>436</a>\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dom.py?line=438'>439</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatcher\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dispatcher.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dispatcher.py?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstarlette\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mendpoints\u001b[39;00m \u001b[39mimport\u001b[39;00m WebSocketEndpoint\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dispatcher.py?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstarlette\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwebsockets\u001b[39;00m \u001b[39mimport\u001b[39;00m WebSocket\n\u001b[0;32m---> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dispatcher.py?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m config, server\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dispatcher.py?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39masync_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m later_await\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/dispatcher.py?line=17'>18</a>\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mreact\u001b[39m\u001b[39m'\u001b[39m]\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=20'>21</a>\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mroute\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrun\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=22'>23</a>\u001b[0m app: Any \u001b[39m=\u001b[39m Starlette(debug\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mDEBUG)\n\u001b[0;32m---> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=23'>24</a>\u001b[0m app\u001b[39m.\u001b[39mmount(config\u001b[39m.\u001b[39mSTATIC_ROUTE, StaticFiles(directory\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mSTATIC_DIRECTORY), name\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mSTATIC_NAME)\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=24'>25</a>\u001b[0m app\u001b[39m.\u001b[39madd_middleware(GZipMiddleware)\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=25'>26</a>\u001b[0m app\u001b[39m.\u001b[39madd_middleware(\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=26'>27</a>\u001b[0m     CORSMiddleware,\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=27'>28</a>\u001b[0m     allow_origins\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=30'>31</a>\u001b[0m     allow_headers\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/frontend/server.py?line=31'>32</a>\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/dev-env/lib/python3.9/site-packages/starlette/staticfiles.py:55\u001b[0m, in \u001b[0;36mStaticFiles.__init__\u001b[0;34m(self, directory, packages, html, check_dir)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/starlette/staticfiles.py?line=52'>53</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig_checked \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/starlette/staticfiles.py?line=53'>54</a>\u001b[0m \u001b[39mif\u001b[39;00m check_dir \u001b[39mand\u001b[39;00m directory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(directory):\n\u001b[0;32m---> <a href='file:///home/ubuntu22/anaconda3/envs/dev-env/lib/python3.9/site-packages/starlette/staticfiles.py?line=54'>55</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDirectory \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdirectory\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Directory 'static/' does not exist"
          ]
        }
      ],
      "source": [
        "import fitz\n",
        "\n",
        "### READ IN PDF\n",
        "doc = fitz.open(\"MasterPython.pdf\")\n",
        "\n",
        "for page in doc:\n",
        "    ### SEARCH\n",
        "    text = \"Python\"\n",
        "    text_instances = page.searchFor(text)\n",
        "\n",
        "    ### HIGHLIGHT\n",
        "    for inst in text_instances:\n",
        "        highlight = page.addHighlightAnnot(inst)\n",
        "        highlight.update()\n",
        "\n",
        "\n",
        "### OUTPUT\n",
        "doc.save(\"outputn.pdf\", garbage=4, deflate=True, clean=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fossil-peoples",
      "metadata": {
        "collapsed": true,
        "id": "fossil-peoples",
        "outputId": "1ede9359-6bed-4ddb-f5e7-eccf330a48ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['FormFonts',\n",
              " 'PDFCatalog',\n",
              " 'PDFTrailer',\n",
              " '__class__',\n",
              " '__contains__',\n",
              " '__del__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__enter__',\n",
              " '__eq__',\n",
              " '__exit__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__swig_destroy__',\n",
              " '__weakref__',\n",
              " '_addFormFont',\n",
              " '_delToC',\n",
              " '_deleteObject',\n",
              " '_delete_page',\n",
              " '_do_links',\n",
              " '_dropOutline',\n",
              " '_embeddedFileGet',\n",
              " '_embeddedFileIndex',\n",
              " '_embfile_add',\n",
              " '_embfile_del',\n",
              " '_embfile_info',\n",
              " '_embfile_names',\n",
              " '_embfile_upd',\n",
              " '_extend_toc_items',\n",
              " '_forget_page',\n",
              " '_getMetadata',\n",
              " '_getOLRootNumber',\n",
              " '_getPDFfileid',\n",
              " '_getPageInfo',\n",
              " '_get_char_widths',\n",
              " '_get_page_labels',\n",
              " '_insert_font',\n",
              " '_loadOutline',\n",
              " '_make_page_map',\n",
              " '_move_copy_page',\n",
              " '_newPage',\n",
              " '_remove_links_to',\n",
              " '_remove_toc_item',\n",
              " '_reset_page_refs',\n",
              " '_set_page_labels',\n",
              " '_update_toc_item',\n",
              " 'add_layer',\n",
              " 'add_ocg',\n",
              " 'authenticate',\n",
              " 'can_save_incrementally',\n",
              " 'chapterCount',\n",
              " 'chapterPageCount',\n",
              " 'chapter_count',\n",
              " 'chapter_page_count',\n",
              " 'close',\n",
              " 'convertToPDF',\n",
              " 'convert_to_pdf',\n",
              " 'copyPage',\n",
              " 'copy_page',\n",
              " 'del_toc_item',\n",
              " 'del_xml_metadata',\n",
              " 'deletePage',\n",
              " 'deletePageRange',\n",
              " 'delete_page',\n",
              " 'delete_pages',\n",
              " 'embeddedFileAdd',\n",
              " 'embeddedFileCount',\n",
              " 'embeddedFileDel',\n",
              " 'embeddedFileGet',\n",
              " 'embeddedFileInfo',\n",
              " 'embeddedFileNames',\n",
              " 'embeddedFileUpd',\n",
              " 'embfile_add',\n",
              " 'embfile_count',\n",
              " 'embfile_del',\n",
              " 'embfile_get',\n",
              " 'embfile_info',\n",
              " 'embfile_names',\n",
              " 'embfile_upd',\n",
              " 'extractFont',\n",
              " 'extractImage',\n",
              " 'extract_font',\n",
              " 'extract_image',\n",
              " 'ez_save',\n",
              " 'findBookmark',\n",
              " 'find_bookmark',\n",
              " 'fullcopyPage',\n",
              " 'fullcopy_page',\n",
              " 'getCharWidths',\n",
              " 'getOCGs',\n",
              " 'getPageFontList',\n",
              " 'getPageImageList',\n",
              " 'getPagePixmap',\n",
              " 'getPageText',\n",
              " 'getPageXObjectList',\n",
              " 'getSigFlags',\n",
              " 'getToC',\n",
              " 'getXmlMetadata',\n",
              " 'get_char_widths',\n",
              " 'get_layer',\n",
              " 'get_layers',\n",
              " 'get_new_xref',\n",
              " 'get_oc',\n",
              " 'get_ocgs',\n",
              " 'get_ocmd',\n",
              " 'get_outline_xrefs',\n",
              " 'get_page_fonts',\n",
              " 'get_page_images',\n",
              " 'get_page_labels',\n",
              " 'get_page_numbers',\n",
              " 'get_page_pixmap',\n",
              " 'get_page_text',\n",
              " 'get_page_xobjects',\n",
              " 'get_sigflags',\n",
              " 'get_toc',\n",
              " 'get_xml_metadata',\n",
              " 'has_annots',\n",
              " 'has_links',\n",
              " 'has_old_style_xrefs',\n",
              " 'has_xref_streams',\n",
              " 'init_doc',\n",
              " 'insertPDF',\n",
              " 'insertPage',\n",
              " 'insert_page',\n",
              " 'insert_pdf',\n",
              " 'isDirty',\n",
              " 'isFormPDF',\n",
              " 'isPDF',\n",
              " 'isReflowable',\n",
              " 'isRepaired',\n",
              " 'isStream',\n",
              " 'is_dirty',\n",
              " 'is_form_pdf',\n",
              " 'is_pdf',\n",
              " 'is_reflowable',\n",
              " 'is_repaired',\n",
              " 'is_stream',\n",
              " 'language',\n",
              " 'lastLocation',\n",
              " 'last_location',\n",
              " 'layer_ui_configs',\n",
              " 'layout',\n",
              " 'loadPage',\n",
              " 'load_page',\n",
              " 'location_from_page_number',\n",
              " 'makeBookmark',\n",
              " 'make_bookmark',\n",
              " 'metadataXML',\n",
              " 'movePage',\n",
              " 'move_page',\n",
              " 'need_appearances',\n",
              " 'needsPass',\n",
              " 'needs_pass',\n",
              " 'newPage',\n",
              " 'new_page',\n",
              " 'nextLocation',\n",
              " 'next_location',\n",
              " 'outline',\n",
              " 'pageCount',\n",
              " 'pageCropBox',\n",
              " 'pageXref',\n",
              " 'page_annot_xrefs',\n",
              " 'page_count',\n",
              " 'page_cropbox',\n",
              " 'page_number_from_location',\n",
              " 'page_xref',\n",
              " 'pages',\n",
              " 'pdf_catalog',\n",
              " 'pdf_trailer',\n",
              " 'permissions',\n",
              " 'prev_location',\n",
              " 'previousLocation',\n",
              " 'reload_page',\n",
              " 'resolveLink',\n",
              " 'resolve_link',\n",
              " 'save',\n",
              " 'saveIncr',\n",
              " 'scrub',\n",
              " 'searchPageFor',\n",
              " 'search_page_for',\n",
              " 'select',\n",
              " 'setLanguage',\n",
              " 'setMetadata',\n",
              " 'setToC',\n",
              " 'setXmlMetadata',\n",
              " 'set_language',\n",
              " 'set_layer',\n",
              " 'set_layer_ui_config',\n",
              " 'set_metadata',\n",
              " 'set_oc',\n",
              " 'set_ocmd',\n",
              " 'set_page_labels',\n",
              " 'set_toc',\n",
              " 'set_toc_item',\n",
              " 'set_xml_metadata',\n",
              " 'subset_fonts',\n",
              " 'switch_layer',\n",
              " 'thisown',\n",
              " 'tobytes',\n",
              " 'updateObject',\n",
              " 'updateStream',\n",
              " 'update_object',\n",
              " 'update_stream',\n",
              " 'write',\n",
              " 'xrefLength',\n",
              " 'xrefObject',\n",
              " 'xrefStream',\n",
              " 'xrefStreamRaw',\n",
              " 'xref_get_key',\n",
              " 'xref_get_keys',\n",
              " 'xref_length',\n",
              " 'xref_object',\n",
              " 'xref_set_key',\n",
              " 'xref_stream',\n",
              " 'xref_stream_raw',\n",
              " 'xref_xml_metadata']"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dir(fitz.open)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "indian-brisbane",
      "metadata": {
        "id": "indian-brisbane"
      },
      "source": [
        "### Hybrid Approach for Fuzzy Name Matching \n",
        "\n",
        "#### Common Issues:\n",
        "1. Companies with varying prefixes or suffixes: PT Indo Tambangraya Megah, Tbk vs. Tamban graya Megah, Indonesia.\n",
        "2. Names that have been abbreviated: MSFT Corp vs. Microsoft Corporation.\n",
        "3. Names that have a region’s name accompanying the core name: Suzhou Synta Optical Technology.\n",
        "4. Names in the system that have been misspelled: Giordano International Limited, HK vs. Gordano Intl’ Ltd., Hong Kong\n",
        "5. Information that exists in the database in varying formats: Key Products — Iron Ore, Copper vs. “…deals with export of iron ore and copper…”\n",
        "6. Names with extra or missing whitespaces: Chu Kong Passenger Transport Co Ltd. vs. ChuKong Passenger Trans. Co.\n",
        "\n",
        "ELTO is an online service and they do not carry out manual searches. If the policy you are looking for is not listed on the simple search results, you can request an extended search via the system which will contact insurers on your behalf. \n",
        "\n",
        "The insurer may need up to 5 weeks for a response to manually search a large volume of data to find the policy you are looking for.\n",
        "\n",
        "\n",
        "* Employer Name\n",
        "* Employer\n",
        "* Parent Company Of the Employer\n",
        "* Exposure Period\n",
        "\n",
        "\n",
        "The bank’s existing process of manual inspection was very good at returning very high-quality matches. The problem was that after the database team filtered the names for quality, the sales team received only tens of customer leads. After we implemented the ensemble fuzzy name matching engine, the results met the same standard of accuracy as the manual process — but increased the number of quality leads 500-fold and produced this increase within a month of the engine’s launch. With the matching engine in place within a total project time of just three months, the bank enjoyed a dramatic leap in the number of leads, required fewer head count to achieve better results, could more efficiently allocate valuable data-scientist time, and experienced lower operational expenses. This article will provide a detailed description of how our BCG team achieved all of these results for our client.\n",
        "\n",
        "1. Phonetic variations: Kohl’s versus Coles\n",
        "2. Typographical mistakes: Microsoft vs. Microsft\n",
        "3. Contextual differences: Company vs. Organization\n",
        "4. Reordered terms: Sam Hopkins vs. Hopkins Sam\n",
        "5. Prefixes and suffixes: AJO Technology Company Limited vs. AJO tech Private Co., Ltd.\n",
        "6. Abbreviations, nicknames, and initials: AJ Wilson vs. Alex Jane Wilson\n",
        "7. Truncated letters and missing or extra spaces: Chu Kong Transport Company vs. ChuKong Transport Co. Ltd.\n",
        "\n",
        "\n",
        "There were five key challenges in building such a data asset:\n",
        "1. Identifying & Match same companies across datasets:\n",
        "2. Normalize company names and addresses across all data sets and bring them into a standard format.\n",
        "3. Remove noise from names in terms of prefixes, suffixes, stop-words, regional addendum and other parameters.\n",
        "\n",
        "Pre-processing textual data:\n",
        "1. Case Normalization\n",
        "2. Toeknization performed at a word or segment level depending on script\n",
        "3. Special characters excluded\n",
        "4. Stemming using Porter Stemmer as default\n",
        "5. Stop word treatment\n",
        "6. A dynamic word/character importance calculator\n",
        "7. List of standard prefixes/suffixes/geographies\n",
        "8. Replacement words\n",
        "\n",
        "\n",
        "### Lets start Fuzzy matching in python - Method1\n",
        "\n",
        "Traditional approaches to string matching such as the Jaro-Winkler or Levenshtein distance measure are too slow for large datasets. \n",
        "Using TF-IDF with N-Grams as terms to find similar strings transforms the problem into a matrix multiplication problem, which is computationally much cheaper. \n",
        "Using this approach made it possible to search for near duplicates in a set of 2M records company names in 1 hrs using a quad-core laptop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "owned-account",
      "metadata": {
        "id": "owned-account",
        "outputId": "114b9691-2fec-42af-af40-a582b2aaa728"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_536/702773374.py:3: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  pd.set_option('display.max_colwidth', -1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape: 663000 x 3\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Line Number</th>\n",
              "      <th>Company Name</th>\n",
              "      <th>Company CIK Key</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>!J INC</td>\n",
              "      <td>1438823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>#1 A LIFESAFER HOLDINGS, INC.</td>\n",
              "      <td>1509607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>#1 ARIZONA DISCOUNT PROPERTIES LLC</td>\n",
              "      <td>1457512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>#1 PAINTBALL CORP</td>\n",
              "      <td>1433777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>$ LLC</td>\n",
              "      <td>1427189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>&amp; S MEDIA GROUP LLC</td>\n",
              "      <td>1447162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>&amp;TV COMMUNICATIONS INC.</td>\n",
              "      <td>1479357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>'MKTG, INC.'</td>\n",
              "      <td>886475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>'OHANA LABS INC.</td>\n",
              "      <td>1703629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>(OURCROWD INVESTMENT IN MST) L.P.</td>\n",
              "      <td>1599496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>(Y.Z) QUEENCO LTD.</td>\n",
              "      <td>1623088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>.CLUB DOMAINS, LLC</td>\n",
              "      <td>1577787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>012 SMILE.COMMUNICATIONS LTD</td>\n",
              "      <td>1402606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>02 MEDTECH INC</td>\n",
              "      <td>1409768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>0210, LLC</td>\n",
              "      <td>1494698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>024 PHARMA, INC.</td>\n",
              "      <td>1307969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>02DIESEL CORP</td>\n",
              "      <td>1414080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>03 ENTERTAINMENT GROUP INC</td>\n",
              "      <td>1322914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>033 ASSET MANAGEMENT LLC /</td>\n",
              "      <td>1114831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>05 AFWAIN COURITY KATHY LLC</td>\n",
              "      <td>1398981</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Line Number                        Company Name  Company CIK Key\n",
              "0   1            !J INC                              1438823        \n",
              "1   2            #1 A LIFESAFER HOLDINGS, INC.       1509607        \n",
              "2   3            #1 ARIZONA DISCOUNT PROPERTIES LLC  1457512        \n",
              "3   4            #1 PAINTBALL CORP                   1433777        \n",
              "4   5            $ LLC                               1427189        \n",
              "5   6            & S MEDIA GROUP LLC                 1447162        \n",
              "6   7            &TV COMMUNICATIONS INC.             1479357        \n",
              "7   8            'MKTG, INC.'                        886475         \n",
              "8   9            'OHANA LABS INC.                    1703629        \n",
              "9   10           (OURCROWD INVESTMENT IN MST) L.P.   1599496        \n",
              "10  11           (Y.Z) QUEENCO LTD.                  1623088        \n",
              "11  12           .CLUB DOMAINS, LLC                  1577787        \n",
              "12  13           012 SMILE.COMMUNICATIONS LTD        1402606        \n",
              "13  14           02 MEDTECH INC                      1409768        \n",
              "14  15           0210, LLC                           1494698        \n",
              "15  16           024 PHARMA, INC.                    1307969        \n",
              "16  17           02DIESEL CORP                       1414080        \n",
              "17  18           03 ENTERTAINMENT GROUP INC          1322914        \n",
              "18  19           033 ASSET MANAGEMENT LLC /          1114831        \n",
              "19  20           05 AFWAIN COURITY KATHY LLC         1398981        "
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "names =  pd.read_csv('sec_edgar_company_info.csv')\n",
        "print('The shape: %d x %d' % names.shape)\n",
        "names.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "temporal-wisdom",
      "metadata": {
        "id": "temporal-wisdom"
      },
      "source": [
        "\n",
        "### N-Grams\n",
        "While the terms in TF-IDF are usually words, this is not a necessity. In our case using words as terms wouldn’t help us much, as most company names only contain one or two words. This is why we will use n-grams: sequences of N contiguous items, in this case characters. The following function cleans a string and generates all n-grams in this string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "processed-horizontal",
      "metadata": {
        "id": "processed-horizontal",
        "outputId": "d47fd944-a53d-4c3f-c758-fc8a15960c28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All 3-grams in \"McDonalds\":\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Mc ', 'c D', ' Do', 'Don', 'ona', 'nal', 'ald', 'lds']"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def ngrams(string, n=3):\n",
        "    string = re.sub(r'[,-./]|\\sBD',r'', string)\n",
        "    ngrams = zip(*[string[i:] for i in range(n)])\n",
        "    return [''.join(ngram) for ngram in ngrams]\n",
        "\n",
        "print('All 3-grams in \"McDonalds\":')\n",
        "ngrams('Mc Donalds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "available-newton",
      "metadata": {
        "id": "available-newton",
        "outputId": "385b9373-7fb0-4bb5-eb0f-adb1a5b9f67b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Mc', 'Donalds'] ['Mc ', 'c D', ' Do', 'Don', 'ona', 'nal', 'ald', 'lds']\n"
          ]
        }
      ],
      "source": [
        "string = re.sub(r'[,-./]|\\sBD',r'', 'Mc Donalds')\n",
        "print(string.split(' '), ngrams('Mc Donalds') )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "logical-habitat",
      "metadata": {
        "id": "logical-habitat"
      },
      "source": [
        "### TF-IDF\n",
        "TF-IDF is a method to generate features from text by multiplying the frequency of a term (usually a word) in a document (the Term Frequency, or TF) by the importance (the Inverse Document Frequency or IDF) of the same term in an entire corpus. This last term weights less important words (e.g. the, it, and etc) down, and words that don’t occur frequently up. IDF is calculated as:\n",
        "\n",
        "IDF(t) = log_e(Total number of documents / Number of documents with term t in it ).\n",
        "\n",
        "An example (from www.tfidf.com/):\n",
        "\n",
        "#### TF:\n",
        "If we take two documents one which have 100 words and other which have 10,000 words. There is a high probability that the common word such as “cat” can be present more in the 10,000 worded document. But we cannot say that the longer document is more important than the shorter document. For this exact reason, we perform a normalization on the frequency value. we divide the the word frequency with the total number of words in the document to get the term frequency (i.e., tf) for cat. \n",
        "Considering a document containing 100 words in which the word cat appears 3 times. tf is then (3 / 100) = 0.03. \n",
        "\n",
        "#### IDF:\n",
        "Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf. Similar to tf, df is no of documents in which the word is present (1000/10,000,000) & idf is inverse of this. Sometimes this number can be high for large no of documents, to dampen this effect we use log) is calculated as log(10,000,000 / 1,000) = 4.\n",
        "\n",
        "###### Note: it is safe to use df+1 in the formula to handle cases when df=0 and avoid division by zero error.  \n",
        "        idf(t) = log(N/(df + 1)) \n",
        "\n",
        "#### TF-IDF:\n",
        "Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
        "\n",
        "TF-IDF is very useful in text classification and text clustering. It is used to transform documents into numeric vectors, that can easily be compared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unlike-delight",
      "metadata": {
        "id": "unlike-delight"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "company_names = names['Company Name']\n",
        "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams)\n",
        "tf_idf_matrix = vectorizer.fit_transform(company_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "serious-purchase",
      "metadata": {
        "id": "serious-purchase",
        "outputId": "64d97ffb-f118-420d-9f28-ba94e67863a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 14549)\t0.15757684989695478\n",
            "  (0, 812)\t0.14545326532967898\n",
            "  (0, 14951)\t0.517420185391102\n",
            "  (0, 1395)\t0.828425757525274\n"
          ]
        }
      ],
      "source": [
        "print(tf_idf_matrix[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "polish-egyptian",
      "metadata": {
        "id": "polish-egyptian",
        "outputId": "1eda1e2b-5578-4ce6-fef2-7bc167acd41a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['!J ', 'J I', ' IN', 'INC']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ngrams('!J INC')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "defined-healthcare",
      "metadata": {
        "id": "defined-healthcare"
      },
      "source": [
        "### Cosine Similarity\n",
        "To calculate the similarity between two vectors of TF-IDF values the Cosine Similarity is usually used. The cosine similarity can be seen as a normalized dot product. For a good explanation see: this site. We can theoretically calculate the cosine similarity of all items in our dataset with all other items in scikit-learn by using the cosine_similarity function, however the Data Scientists at ING found out this has some disadvantages:\n",
        "* The sklearn version does a lot of type checking and error handling.\n",
        "* The sklearn version calculates and stores all similarities in one go, while we are only interested in the most similar ones. Therefore it uses a lot more memory than necessary.\n",
        "\n",
        "To optimize for these disadvantages they created their own library which stores only the top N highest matches in each row, and only the similarities above an (optional) threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "driving-aerospace",
      "metadata": {
        "id": "driving-aerospace",
        "outputId": "565950a5-adee-43e1-8932-1c593b45f18f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sparse_dot_topn\n",
            "  Downloading sparse_dot_topn-0.2.9.tar.gz (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 8.3 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: cython>=0.29.15 in /usr/local/lib/python3.8/dist-packages (from sparse_dot_topn) (0.29.23)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from sparse_dot_topn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from sparse_dot_topn) (1.6.2)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/lib/python3/dist-packages (from sparse_dot_topn) (45.2.0)\n",
            "Building wheels for collected packages: sparse-dot-topn\n",
            "  Building wheel for sparse-dot-topn (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sparse-dot-topn: filename=sparse_dot_topn-0.2.9-cp38-cp38-linux_x86_64.whl size=397613 sha256=2af189ee08e543b174623f29962e7691b0397c94849139de95ae73f0a8fb5d68\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/64/25/2e1cedbf7513fecda8d396a0431f3a1e217f45f1ab565e8ce8\n",
            "Successfully built sparse-dot-topn\n",
            "Installing collected packages: sparse-dot-topn\n",
            "Successfully installed sparse-dot-topn-0.2.9\n"
          ]
        }
      ],
      "source": [
        "!pip3 install sparse_dot_topn\n",
        "On my Ubuntu machine with python 3.8.5, the issue was resolved by:\n",
        "apt-get update\n",
        "apt-get install -y pkg-config\n",
        "pip install sparse_dot_topn\n",
        "\n",
        "If it does not work - try updating pip and setup tools also\n",
        "pip install --upgrade pip setuptools wheel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb7d0d58",
      "metadata": {
        "id": "bb7d0d58"
      },
      "source": [
        "On my Ubuntu machine with python 3.8.5, the issue was resolved by:\n",
        "apt-get update\n",
        "apt-get install -y pkg-config\n",
        "pip install sparse_dot_topn\n",
        "\n",
        "If it does not work - try updating pip and setup tools also\n",
        "pip install --upgrade pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "immediate-xerox",
      "metadata": {
        "id": "immediate-xerox",
        "outputId": "cc597020-c379-4702-86bc-41db4b1a9570"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sparse_dot_topn'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/mnt/d/JupyterNotebooks/Extract Text From PDF and search .ipynb Cell 32'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://127.0.0.1:8080/mnt/d/JupyterNotebooks/Extract%20Text%20From%20PDF%20and%20search%20.ipynb#ch0000030vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://127.0.0.1:8080/mnt/d/JupyterNotebooks/Extract%20Text%20From%20PDF%20and%20search%20.ipynb#ch0000030vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m csr_matrix\n\u001b[0;32m----> <a href='vscode-notebook-cell://127.0.0.1:8080/mnt/d/JupyterNotebooks/Extract%20Text%20From%20PDF%20and%20search%20.ipynb#ch0000030vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msparse_dot_topn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse_dot_topn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mct\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://127.0.0.1:8080/mnt/d/JupyterNotebooks/Extract%20Text%20From%20PDF%20and%20search%20.ipynb#ch0000030vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mawesome_cossim_top\u001b[39m(A, B, ntop, lower_bound\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://127.0.0.1:8080/mnt/d/JupyterNotebooks/Extract%20Text%20From%20PDF%20and%20search%20.ipynb#ch0000030vscode-remote?line=5'>6</a>\u001b[0m     \u001b[39m# force A and B as a CSR matrix.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://127.0.0.1:8080/mnt/d/JupyterNotebooks/Extract%20Text%20From%20PDF%20and%20search%20.ipynb#ch0000030vscode-remote?line=6'>7</a>\u001b[0m     \u001b[39m# If they have already been CSR, there is no overhead\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://127.0.0.1:8080/mnt/d/JupyterNotebooks/Extract%20Text%20From%20PDF%20and%20search%20.ipynb#ch0000030vscode-remote?line=7'>8</a>\u001b[0m     A \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mtocsr()\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sparse_dot_topn'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "import sparse_dot_topn.sparse_dot_topn as ct\n",
        "\n",
        "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
        "    # force A and B as a CSR matrix.\n",
        "    # If they have already been CSR, there is no overhead\n",
        "    A = A.tocsr()\n",
        "    B = B.tocsr()\n",
        "    M, _ = A.shape\n",
        "    _, N = B.shape\n",
        " \n",
        "    idx_dtype = np.int32\n",
        " \n",
        "    nnz_max = M*ntop\n",
        " \n",
        "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
        "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
        "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
        "\n",
        "    ct.sparse_dot_topn(\n",
        "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
        "        np.asarray(A.indices, dtype=idx_dtype),\n",
        "        A.data,\n",
        "        np.asarray(B.indptr, dtype=idx_dtype),\n",
        "        np.asarray(B.indices, dtype=idx_dtype),\n",
        "        B.data,\n",
        "        ntop,\n",
        "        lower_bound,\n",
        "        indptr, indices, data)\n",
        "\n",
        "    return csr_matrix((data,indices,indptr),shape=(M,N))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ideal-boost",
      "metadata": {
        "id": "ideal-boost",
        "outputId": "b15c5532-da4e-450a-be6d-e8ebae0d35bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SELFTIMED: 1460.2296192646027\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "t1 = time.time()\n",
        "matches = awesome_cossim_top(tf_idf_matrix, tf_idf_matrix.transpose(), 10, 0.8)\n",
        "t = time.time()-t1\n",
        "print(\"SELFTIMED:\", t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sunrise-aircraft",
      "metadata": {
        "id": "sunrise-aircraft",
        "outputId": "e1aaf647-7e96-4e15-8309-0aeba6deab2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 14549)\t0.15757684989695478\n",
            "  (0, 812)\t0.14545326532967898\n",
            "  (0, 14951)\t0.517420185391102\n",
            "  (0, 1395)\t0.828425757525274\n"
          ]
        }
      ],
      "source": [
        "print(tf_idf_matrix[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "occupied-cycling",
      "metadata": {
        "id": "occupied-cycling",
        "outputId": "c05b9d0d-ee79-49c7-d686-6a31a06a1db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 173)\t0.9358812989810146\n",
            "  (0, 1006)\t0.9822409862138929\n",
            "  (0, 1366)\t0.976074306631421\n",
            "  (0, 1380)\t0.9282378903388704\n",
            "  (0, 1389)\t0.9917511712168043\n",
            "  (0, 1390)\t0.9923096093432691\n",
            "  (0, 1391)\t0.993664581503392\n",
            "  (0, 2036)\t0.9922161076729631\n",
            "  (0, 2042)\t0.9902886343379361\n",
            "  (0, 2337)\t0.9908775675274805\n",
            "  (0, 2909)\t0.9900331577861996\n",
            "  (0, 2974)\t0.9958528399428778\n",
            "  (0, 3394)\t0.9923111267867728\n",
            "  (0, 3562)\t0.9669048466500032\n",
            "  (0, 4358)\t0.9915313755942311\n",
            "  (0, 4374)\t0.9936671348171857\n",
            "  (0, 4376)\t0.9942949690131819\n",
            "  (0, 4385)\t0.993773986279108\n",
            "  (0, 4398)\t0.9857493054687682\n",
            "  (0, 4404)\t0.993193689190375\n",
            "  (0, 4407)\t0.995200246784032\n",
            "  (0, 4411)\t0.9899307855639606\n",
            "  (0, 4420)\t0.9781015043875542\n",
            "  (0, 4427)\t0.993948613630555\n",
            "  (0, 4437)\t0.993298035813958\n",
            "  :\t:\n",
            "  (0, 659366)\t0.9940310120666993\n",
            "  (0, 659424)\t0.9950498586575056\n",
            "  (0, 659620)\t0.9904277696427688\n",
            "  (0, 660448)\t0.9930700682958207\n",
            "  (0, 660479)\t0.9943961643207411\n",
            "  (0, 660652)\t0.9547839673442653\n",
            "  (0, 660703)\t0.9853631704399891\n",
            "  (0, 660759)\t0.9870923893810459\n",
            "  (0, 660760)\t0.9875823656262646\n",
            "  (0, 661428)\t0.991212193655022\n",
            "  (0, 661429)\t0.9857488296173974\n",
            "  (0, 661623)\t0.9730160039586409\n",
            "  (0, 661635)\t0.9917013588358442\n",
            "  (0, 661756)\t0.9883013525443228\n",
            "  (0, 661919)\t0.8466916789701178\n",
            "  (0, 661923)\t0.9808928307631225\n",
            "  (0, 661924)\t0.9818706241715853\n",
            "  (0, 661925)\t0.9842484023741918\n",
            "  (0, 661930)\t0.979085342562955\n",
            "  (0, 662294)\t0.9885921402373985\n",
            "  (0, 662449)\t0.9706832567365764\n",
            "  (0, 662530)\t0.2025176637594865\n",
            "  (0, 662582)\t0.9913611976471147\n",
            "  (0, 662769)\t0.984240224298937\n",
            "  (0, 662960)\t0.1958655188687881\n"
          ]
        }
      ],
      "source": [
        "print(tf_idf_matrix.transpose()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "entire-bradford",
      "metadata": {
        "id": "entire-bradford"
      },
      "outputs": [],
      "source": [
        "def get_matches_df(sparse_matrix, name_vector, top=100):\n",
        "    non_zeros = sparse_matrix.nonzero()\n",
        "    \n",
        "    sparserows = non_zeros[0]\n",
        "    sparsecols = non_zeros[1]\n",
        "    \n",
        "    if top:\n",
        "        nr_matches = top\n",
        "    else:\n",
        "        nr_matches = sparsecols.size\n",
        "    \n",
        "    left_side = np.empty([nr_matches], dtype=object)\n",
        "    right_side = np.empty([nr_matches], dtype=object)\n",
        "    similairity = np.zeros(nr_matches)\n",
        "    \n",
        "    for index in range(0, nr_matches):\n",
        "        left_side[index] = name_vector[sparserows[index]]\n",
        "        right_side[index] = name_vector[sparsecols[index]]\n",
        "        similairity[index] = sparse_matrix.data[index]\n",
        "    \n",
        "    return pd.DataFrame({'left_side': left_side,\n",
        "                          'right_side': right_side,\n",
        "                           'similairity': similairity})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "usual-contest",
      "metadata": {
        "id": "usual-contest",
        "outputId": "18626af5-759a-4ffe-a025-16883fccca86"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>left_side</th>\n",
              "      <th>right_side</th>\n",
              "      <th>similairity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42110</th>\n",
              "      <td>AEI 2007 VENTURE INVESTMENTS II LLC</td>\n",
              "      <td>AEI 2007 VENTURE INVESTMENTS III LLC</td>\n",
              "      <td>0.987310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36692</th>\n",
              "      <td>ADVISORS DISCIPLINED TRUST 626</td>\n",
              "      <td>ADVISORS DISCIPLINED TRUST 629</td>\n",
              "      <td>0.873253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40494</th>\n",
              "      <td>ADVISORS DISCIPLINED TRUST 972</td>\n",
              "      <td>ADVISORS DISCIPLINED TRUST 977</td>\n",
              "      <td>0.879738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90240</th>\n",
              "      <td>ARES ICOF III MANAGEMENT LP</td>\n",
              "      <td>ARES ICOF I MANAGEMENT, LLC</td>\n",
              "      <td>0.907832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77125</th>\n",
              "      <td>ANDERSON TIMOTHY B</td>\n",
              "      <td>ANDERSON TIMOTHY R</td>\n",
              "      <td>0.923840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38879</th>\n",
              "      <td>ADVISORS DISCIPLINED TRUST 826</td>\n",
              "      <td>ADVISORS DISCIPLINED TRUST 822</td>\n",
              "      <td>0.871160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34512</th>\n",
              "      <td>ADVISORS DISCIPLINED TRUST 428</td>\n",
              "      <td>ADVISORS DISCIPLINED TRUST 421</td>\n",
              "      <td>0.873709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9980</th>\n",
              "      <td>ABN AMRO MORTGAGE CORP SERIES 2002-4</td>\n",
              "      <td>ABN AMRO MORTGAGE CORP SERIES 2002-3</td>\n",
              "      <td>0.909712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29227</th>\n",
              "      <td>ADVISORS DISCIPLINED TRUST 1576</td>\n",
              "      <td>ADVISORS DISCIPLINED TRUST 1575</td>\n",
              "      <td>0.883230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90330</th>\n",
              "      <td>ARES MANAGEMENT INC</td>\n",
              "      <td>ARES MANAGEMENT LP</td>\n",
              "      <td>0.884954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92283</th>\n",
              "      <td>ARK PACIFIC INVESTMENT MANAGEMENT LTD</td>\n",
              "      <td>PACIFIC INVESTMENT MANAGEMENT CO LLC</td>\n",
              "      <td>0.843027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89918</th>\n",
              "      <td>ARES EMPLOYEE PARTICIPATION V, LLC</td>\n",
              "      <td>ARES EMPLOYEE PARTICIPATION III, LLC</td>\n",
              "      <td>0.871506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94131</th>\n",
              "      <td>ARROWHEAD INVESTORS LLC</td>\n",
              "      <td>ARROWHEAD II LLC</td>\n",
              "      <td>0.800831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37224</th>\n",
              "      <td>ADVISORS DISCIPLINED TRUST 675</td>\n",
              "      <td>ADVISORS DISCIPLINED TRUST 673</td>\n",
              "      <td>0.868066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52487</th>\n",
              "      <td>ALERUS FINANCIAL NA  /TA</td>\n",
              "      <td>ALERUS FINANCIAL NA /TA</td>\n",
              "      <td>0.872455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29721</th>\n",
              "      <td>ADVISORS DISCIPLINED TRUST 1620</td>\n",
              "      <td>ADVISORS DISCIPLINED TRUST 1624</td>\n",
              "      <td>0.886826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43191</th>\n",
              "      <td>AETHER REAL ASSETS IV, L.P.</td>\n",
              "      <td>AETHER REAL ASSETS I, L.P.</td>\n",
              "      <td>0.870460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98517</th>\n",
              "      <td>ASSET BACKED FLOATING RATE CERTIFICATES SERIES 1998-OPT1</td>\n",
              "      <td>ASSET BACKED FLOATING RATE CERTIFICATES SERIES 1998-OPT2</td>\n",
              "      <td>0.936569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91920</th>\n",
              "      <td>ARISTARC ABSOLUTE RETURN FUND LTD.</td>\n",
              "      <td>SC ABSOLUTE RETURN FUND, LLC</td>\n",
              "      <td>0.810941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72550</th>\n",
              "      <td>AMRESCO RESIDENTIAL SECURITIES CORP MORT LOAN TRUST 1996-1</td>\n",
              "      <td>AMRESCO RESIDENTIAL SECS CORP MORT LOAN TRUST 1998-3</td>\n",
              "      <td>0.805957</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                        left_side  \\\n",
              "42110  AEI 2007 VENTURE INVESTMENTS II LLC                          \n",
              "36692  ADVISORS DISCIPLINED TRUST 626                               \n",
              "40494  ADVISORS DISCIPLINED TRUST 972                               \n",
              "90240  ARES ICOF III MANAGEMENT LP                                  \n",
              "77125  ANDERSON TIMOTHY B                                           \n",
              "38879  ADVISORS DISCIPLINED TRUST 826                               \n",
              "34512  ADVISORS DISCIPLINED TRUST 428                               \n",
              "9980   ABN AMRO MORTGAGE CORP SERIES 2002-4                         \n",
              "29227  ADVISORS DISCIPLINED TRUST 1576                              \n",
              "90330  ARES MANAGEMENT INC                                          \n",
              "92283  ARK PACIFIC INVESTMENT MANAGEMENT LTD                        \n",
              "89918  ARES EMPLOYEE PARTICIPATION V, LLC                           \n",
              "94131  ARROWHEAD INVESTORS LLC                                      \n",
              "37224  ADVISORS DISCIPLINED TRUST 675                               \n",
              "52487  ALERUS FINANCIAL NA  /TA                                     \n",
              "29721  ADVISORS DISCIPLINED TRUST 1620                              \n",
              "43191  AETHER REAL ASSETS IV, L.P.                                  \n",
              "98517  ASSET BACKED FLOATING RATE CERTIFICATES SERIES 1998-OPT1     \n",
              "91920  ARISTARC ABSOLUTE RETURN FUND LTD.                           \n",
              "72550  AMRESCO RESIDENTIAL SECURITIES CORP MORT LOAN TRUST 1996-1   \n",
              "\n",
              "                                                     right_side  similairity  \n",
              "42110  AEI 2007 VENTURE INVESTMENTS III LLC                      0.987310     \n",
              "36692  ADVISORS DISCIPLINED TRUST 629                            0.873253     \n",
              "40494  ADVISORS DISCIPLINED TRUST 977                            0.879738     \n",
              "90240  ARES ICOF I MANAGEMENT, LLC                               0.907832     \n",
              "77125  ANDERSON TIMOTHY R                                        0.923840     \n",
              "38879  ADVISORS DISCIPLINED TRUST 822                            0.871160     \n",
              "34512  ADVISORS DISCIPLINED TRUST 421                            0.873709     \n",
              "9980   ABN AMRO MORTGAGE CORP SERIES 2002-3                      0.909712     \n",
              "29227  ADVISORS DISCIPLINED TRUST 1575                           0.883230     \n",
              "90330  ARES MANAGEMENT LP                                        0.884954     \n",
              "92283  PACIFIC INVESTMENT MANAGEMENT CO LLC                      0.843027     \n",
              "89918  ARES EMPLOYEE PARTICIPATION III, LLC                      0.871506     \n",
              "94131  ARROWHEAD II LLC                                          0.800831     \n",
              "37224  ADVISORS DISCIPLINED TRUST 673                            0.868066     \n",
              "52487  ALERUS FINANCIAL NA /TA                                   0.872455     \n",
              "29721  ADVISORS DISCIPLINED TRUST 1624                           0.886826     \n",
              "43191  AETHER REAL ASSETS I, L.P.                                0.870460     \n",
              "98517  ASSET BACKED FLOATING RATE CERTIFICATES SERIES 1998-OPT2  0.936569     \n",
              "91920  SC ABSOLUTE RETURN FUND, LLC                              0.810941     \n",
              "72550  AMRESCO RESIDENTIAL SECS CORP MORT LOAN TRUST 1998-3      0.805957     "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "matches_df = get_matches_df(matches, company_names, top=100000)\n",
        "matches_df = matches_df[matches_df['similairity'] < 0.99999] # Remove all exact matches\n",
        "matches_df = matches_df.sort_values(['similairity'], ascending=False)\n",
        "matches_df.sample(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "patent-amateur",
      "metadata": {
        "id": "patent-amateur",
        "outputId": "93a18657-c957-4ff8-cb2e-240f6c170a89"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>left_side</th>\n",
              "      <th>right_side</th>\n",
              "      <th>similairity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>54674</th>\n",
              "      <td>ALLEN &amp; CO INC                                          /BD</td>\n",
              "      <td>MULLEN &amp; CO INC                                         /BD</td>\n",
              "      <td>0.998930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5861</th>\n",
              "      <td>A P SECURITIES INC                                      /BD</td>\n",
              "      <td>J P SECURITIES INC                                      /BD</td>\n",
              "      <td>0.998636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47650</th>\n",
              "      <td>AGS SECURITIES INC                                      /BD</td>\n",
              "      <td>CS SECURITIES INC                                       /BD</td>\n",
              "      <td>0.998415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99706</th>\n",
              "      <td>ASTOR SECURITIES INC                                    /BD</td>\n",
              "      <td>MENTOR SECURITIES INC                                   /BD</td>\n",
              "      <td>0.998364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63516</th>\n",
              "      <td>AM CAPITAL LLC                                          /BD</td>\n",
              "      <td>ML CAPITAL LLC                                          /BD</td>\n",
              "      <td>0.998340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41599</th>\n",
              "      <td>AE PARTNERS LLC                                         /BD</td>\n",
              "      <td>LANE PARTNERS LLC                                       /BD</td>\n",
              "      <td>0.998318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48229</th>\n",
              "      <td>AI SECURITIES INC                                       /BD</td>\n",
              "      <td>CS SECURITIES INC                                       /BD</td>\n",
              "      <td>0.998301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48230</th>\n",
              "      <td>AI SECURITIES INC                                       /BD</td>\n",
              "      <td>USI SECURITIES INC                                      /BD</td>\n",
              "      <td>0.998287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54147</th>\n",
              "      <td>ALL IN LLC                                              /BD</td>\n",
              "      <td>WYN LLC                                                 /BD</td>\n",
              "      <td>0.998228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63517</th>\n",
              "      <td>AM CAPITAL LLC                                          /BD</td>\n",
              "      <td>THOR CAPITAL LLC                                        /BD</td>\n",
              "      <td>0.998185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                         left_side  \\\n",
              "54674  ALLEN & CO INC                                          /BD   \n",
              "5861   A P SECURITIES INC                                      /BD   \n",
              "47650  AGS SECURITIES INC                                      /BD   \n",
              "99706  ASTOR SECURITIES INC                                    /BD   \n",
              "63516  AM CAPITAL LLC                                          /BD   \n",
              "41599  AE PARTNERS LLC                                         /BD   \n",
              "48229  AI SECURITIES INC                                       /BD   \n",
              "48230  AI SECURITIES INC                                       /BD   \n",
              "54147  ALL IN LLC                                              /BD   \n",
              "63517  AM CAPITAL LLC                                          /BD   \n",
              "\n",
              "                                                        right_side  \\\n",
              "54674  MULLEN & CO INC                                         /BD   \n",
              "5861   J P SECURITIES INC                                      /BD   \n",
              "47650  CS SECURITIES INC                                       /BD   \n",
              "99706  MENTOR SECURITIES INC                                   /BD   \n",
              "63516  ML CAPITAL LLC                                          /BD   \n",
              "41599  LANE PARTNERS LLC                                       /BD   \n",
              "48229  CS SECURITIES INC                                       /BD   \n",
              "48230  USI SECURITIES INC                                      /BD   \n",
              "54147  WYN LLC                                                 /BD   \n",
              "63517  THOR CAPITAL LLC                                        /BD   \n",
              "\n",
              "       similairity  \n",
              "54674  0.998930     \n",
              "5861   0.998636     \n",
              "47650  0.998415     \n",
              "99706  0.998364     \n",
              "63516  0.998340     \n",
              "41599  0.998318     \n",
              "48229  0.998301     \n",
              "48230  0.998287     \n",
              "54147  0.998228     \n",
              "63517  0.998185     "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "matches_df.sort_values(['similairity'], ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "numerical-engineer",
      "metadata": {
        "id": "numerical-engineer"
      },
      "source": [
        "### Conclusion\n",
        "As we saw by visual inspection the matches created with this method are quite good, as the strings are very similar. The biggest advantage however, is the speed. The method described above can be scaled to much larger datasets by using a distributed computing environment such as Apache Spark. This could be done by broadcasting one of the TF-IDF matrices to all workers, and parallelizing the second (in our case a copy of the TF-IDF matrix) into multiple sub-matrices. Multiplication can then be done (using Numpy or the sparse_dot_topn library) by each worker on part of the second matrix and the entire first matrix. An example of this is described here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "informed-split",
      "metadata": {
        "id": "informed-split"
      },
      "source": [
        "### Lets start Fuzzy matching in python - Method2\n",
        "\n",
        "#### Common Issues:\n",
        "1. Companies with varying prefixes or suffixes: PT Indo Tambangraya Megah, Tbk vs. Tamban graya Megah, Indonesia.\n",
        "2. Names that have been abbreviated: MSFT Corp vs. Microsoft Corporation.\n",
        "3. Names that have a region’s name accompanying the core name: Suzhou Synta Optical Technology.\n",
        "4. Names in the system that have been misspelled: Giordano International Limited, HK vs. Gordano Intl’ Ltd., Hong Kong\n",
        "5. Information that exists in the database in varying formats: Key Products — Iron Ore, Copper vs. “…deals with export of iron ore and copper…”\n",
        "6. Names with extra or missing whitespaces: Chu Kong Passenger Transport Co Ltd. vs. ChuKong Passenger Trans. Co.\n",
        "\n",
        "ELTO is an online service and we do not carry out manual searches. If the policy you are looking for is not listed on the simple search results, you can request an extended search via the system which will contact insurers on your behalf. \n",
        "\n",
        "The insurer may need up to 5 weeks for a response to manually search a large volume of data to find the policy you are looking for.\n",
        "\n",
        "\n",
        "\n",
        "* Employer Name\n",
        "* Employer\n",
        "* Parent Company Of the Employer\n",
        "* Exposure Period\n",
        "\n",
        "\n",
        "The bank’s existing process of manual inspection was very good at returning very high-quality matches. The problem was that after the database team filtered the names for quality, the sales team received only tens of customer leads. After we implemented the ensemble fuzzy name matching engine, the results met the same standard of accuracy as the manual process — but increased the number of quality leads 500-fold and produced this increase within a month of the engine’s launch. With the matching engine in place within a total project time of just three months, the bank enjoyed a dramatic leap in the number of leads, required fewer head count to achieve better results, could more efficiently allocate valuable data-scientist time, and experienced lower operational expenses. This article will provide a detailed description of how our BCG team achieved all of these results for our client.\n",
        "\n",
        "1. Phonetic variations: Kohl’s versus Coles\n",
        "2. Typographical mistakes: Microsoft vs. Microsft\n",
        "3. Contextual differences: Company vs. Organization\n",
        "4. Reordered terms: Sam Hopkins vs. Hopkins Sam\n",
        "5. Prefixes and suffixes: AJO Technology Company Limited vs. AJO tech Private Co., Ltd.\n",
        "6. Abbreviations, nicknames, and initials: AJ Wilson vs. Alex Jane Wilson\n",
        "7. Truncated letters and missing or extra spaces: Chu Kong Transport Company vs. ChuKong Transport Co. Ltd.\n",
        "\n",
        "\n",
        "There were five key challenges in building such a data asset:\n",
        "1. Identifying & Match same companies across datasets:\n",
        "2. Normalize company names and addresses across all data sets and bring them into a standard format.\n",
        "3. Remove noise from names in terms of prefixes, suffixes, stop-words, regional addendum and other parameters.\n",
        "\n",
        "Pre-processing textual data:\n",
        "1. Case Normalization\n",
        "2. Toeknization performed at a word or segment level depending on script\n",
        "3. Special characters excluded\n",
        "4. Stemming using Porter Stemmer as default\n",
        "5. Stop word treatment\n",
        "6. A dynamic word/character importance calculator\n",
        "7. List of standard prefixes/suffixes/geographies\n",
        "8. Replacement words\n",
        "\n",
        "### TF-IDF\n",
        "TF-IDF is a method to generate features from text by multiplying the frequency of a term (usually a word) in a document (the Term Frequency, or TF) by the importance (the Inverse Document Frequency or IDF) of the same term in an entire corpus. This last term weights less important words (e.g. the, it, and etc) down, and words that don’t occur frequently up. IDF is calculated as:\n",
        "\n",
        "IDF(t) = log_e(Total number of documents / Number of documents with term t in it ).\n",
        "\n",
        "An example (from www.tfidf.com/):\n",
        "\n",
        "#### TF:\n",
        "If we take two documents one which have 100 words and other which have 10,000 words. There is a high probability that the common word such as “cat” can be present more in the 10,000 worded document. But we cannot say that the longer document is more important than the shorter document. For this exact reason, we perform a normalization on the frequency value. we divide the the word frequency with the total number of words in the document to get the term frequency (i.e., tf) for cat. \n",
        "Considering a document containing 100 words in which the word cat appears 3 times. tf is then (3 / 100) = 0.03. \n",
        "\n",
        "#### IDF:\n",
        "Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf. Similar to tf, df is no of documents in which the word is present (1000/10,000,000) & idf is inverse of this. Sometimes this number can be high for large no of documents, to dampen this effect we use log) is calculated as log(10,000,000 / 1,000) = 4.\n",
        "\n",
        "###### Note: it is safe to use df+1 in the formula to handle cases when df=0 and avoid division by zero error.  \n",
        "        idf(t) = log(N/(df + 1)) \n",
        "\n",
        "#### TF-IDF:\n",
        "Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
        "\n",
        "TF-IDF is very useful in text classification and text clustering. It is used to transform documents into numeric vectors, that can easily be compared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mineral-ratio",
      "metadata": {
        "id": "mineral-ratio"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "dev-env",
      "language": "python",
      "name": "dev-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "Extract Text From PDF and search .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "indian-brisbane",
        "temporal-wisdom",
        "logical-habitat",
        "defined-healthcare",
        "numerical-engineer",
        "informed-split"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}